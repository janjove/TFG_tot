{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crear_dataset import *\n",
    "from funcions_net import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyreadstat\n",
    "import pandas as pd\n",
    "from preprocessing import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from funcions_net import *\n",
    "from datasets_separats import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creant dataset inicial\n",
      "Creant dataset origen\n",
      "(72, 15)\n",
      "Creant dataset origen\n",
      "(72, 37)\n",
      "Delictes comesos\n",
      "(72, 15)\n",
      "Dataset final\n",
      "(72, 15)\n",
      "Dataset final\n",
      "(72, 53)\n",
      "Violent_o_lesions\n",
      "False    40\n",
      "True     32\n",
      "Name: count, dtype: int64\n",
      "Index(['temps_fins_reincidencia1a', 'YPI01', 'YPI02', 'YPI03', 'YPI04',\n",
      "       'YPI05', 'YPI06', 'YPI07', 'YPI08', 'YPI09', 'YPI10', 'YPI11', 'YPI12',\n",
      "       'YPI13', 'YPI14', 'YPI15', 'YPI16', 'YPI17', 'YPI18', 'YPI19', 'YPI20',\n",
      "       'YPI21', 'YPI22', 'YPI23', 'YPI24', 'YPI25', 'YPI26', 'YPI27', 'YPI28',\n",
      "       'YPI29', 'YPI30', 'YPI31', 'YPI32', 'YPI33', 'YPI34', 'YPI35', 'YPI36',\n",
      "       'YPI37', 'YPI38', 'YPI39', 'YPI40', 'YPI41', 'YPI42', 'YPI43', 'YPI44',\n",
      "       'YPI45', 'YPI46', 'YPI47', 'YPI48', 'YPI49', 'YPI50', 'xYPI23',\n",
      "       'xYPI35', 'xYPI49', 'YPI_DC', 'YPI_GR', 'YPI_LY', 'YPI_MA', 'YPI_RE',\n",
      "       'YPI_UN', 'YPI_CA', 'YPI_TS', 'YPI_IM', 'YPI_IR', 'YPI_PT', 'YPI_GM',\n",
      "       'YPI_CU', 'YPI_II', 'YPI_PT_3gr', 'YPI_GM_3gr', 'YPI_CU_3gr',\n",
      "       'YPI_II_3gr', 'YPI_PT_2gr', 'YPI_GM_2gr', 'YPI_CU_2gr', 'YPI_II_2gr'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\crear_dataset.py:158: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_actual['Gitanos'] = np.where(df_anterior['Etnia'] ==1, 1, 0)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\crear_dataset.py:159: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_actual['Magrebins'] = np.where(df_anterior['Etnia'] ==2, 1, 0)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\crear_dataset.py:160: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_actual['Subsaharians'] = np.where(df_anterior['Etnia'] ==3, 1, 0)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\crear_dataset.py:161: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_actual['Llatins'] = np.where(df_anterior['Etnia'] ==4, 1, 0)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\crear_dataset.py:162: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_actual['Caucasics'] = np.where(df_anterior['Etnia'] ==5, 1, 0)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\crear_dataset.py:163: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_actual['Asiatics'] = np.where(df_anterior['Etnia'] ==6, 1, 0)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\crear_dataset.py:164: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_actual['Eslaus'] = np.where(df_anterior['Etnia'] ==7, 1, 0)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\crear_dataset.py:166: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_actual['Espanya'] = np.where(df_anterior['nacionalitat_agrupat'] == 1, 1, 0)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\crear_dataset.py:167: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_actual['Unio_Europea'] = np.where(df_anterior['nacionalitat_agrupat'] == 2, 1, 0)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\crear_dataset.py:168: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_actual['Resta_Europa'] = np.where(df_anterior['nacionalitat_agrupat'] == 3, 1, 0)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\crear_dataset.py:169: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_actual['Magreb'] = np.where(df_anterior['nacionalitat_agrupat'] == 4, 1, 0)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\crear_dataset.py:170: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_actual['Resta_Africa'] = np.where(df_anterior['nacionalitat_agrupat'] == 5, 1, 0)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\crear_dataset.py:171: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_actual['Centre_Sud_America'] = np.where(df_anterior['nacionalitat_agrupat'] == 6, 1, 0)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\crear_dataset.py:172: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_actual['Asia'] = np.where(df_anterior['nacionalitat_agrupat'] == 7, 1, 0)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\crear_dataset.py:173: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_actual['Resta_Mon'] = np.where(df_anterior['nacionalitat_agrupat'] == 8, 1, 0)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\crear_dataset.py:174: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_actual['Nord_America'] = np.where(df_anterior['nacionalitat_agrupat'] == 9, 1, 0)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\crear_dataset.py:176: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_actual['Pares_Catalans'] = np.where(df_anterior['OrgnFam'] == 1, 1, 0)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\crear_dataset.py:177: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_actual['Pare_Catala_Altres_Espanya'] = np.where(df_anterior['OrgnFam'] == 2, 1, 0)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\crear_dataset.py:178: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_actual['Pare_Catala_Fora_Espanya'] = np.where(df_anterior['OrgnFam'] == 3, 1, 0)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\crear_dataset.py:179: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_actual['Pares_No_Catalans_Espanya'] = np.where(df_anterior['OrgnFam'] == 4, 1, 0)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\crear_dataset.py:180: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_actual['Pares_No_Catalans_Un_Espanya'] = np.where(df_anterior['OrgnFam'] == 5, 1, 0)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\crear_dataset.py:181: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_actual['Pares_Fora_Espanya'] = np.where(df_anterior['OrgnFam'] == 6, 1, 0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_orig, meta = pyreadstat.read_sav(\"CEJFEAjut2015Updated.sav\")\n",
    "\n",
    "df_variables = pd.read_csv(\"variables.csv\", sep=';')\n",
    "\n",
    "dict = create_dict(meta)\n",
    "\n",
    "datasets = dataset_dif2(df_orig, dict,df_variables)\n",
    "\n",
    "df_inicial = dataset_inicial(df_orig, dict)\n",
    "\n",
    "print(df_inicial[\"Violent_o_lesions\"].value_counts())\n",
    "\n",
    "print(datasets[\"YPI\"].columns)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jjf20\\AppData\\Local\\Temp\\ipykernel_9264\\283642375.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[name]['Violent_o_lesions'] = df_inicial['Violent_o_lesions']\n",
      "C:\\Users\\jjf20\\AppData\\Local\\Temp\\ipykernel_9264\\283642375.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[name]['Violent_o_lesions'] = df_inicial['Violent_o_lesions']\n",
      "C:\\Users\\jjf20\\AppData\\Local\\Temp\\ipykernel_9264\\283642375.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[name]['Violent_o_lesions'] = df_inicial['Violent_o_lesions']\n",
      "C:\\Users\\jjf20\\AppData\\Local\\Temp\\ipykernel_9264\\283642375.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[name]['Violent_o_lesions'] = df_inicial['Violent_o_lesions']\n"
     ]
    }
   ],
   "source": [
    "for name in datasets:\n",
    "    datasets[name]['Violent_o_lesions'] = df_inicial['Violent_o_lesions']\n",
    "    if 'temps_fins_reincidencia1a' in datasets[name].columns:\n",
    "        datasets[name] = datasets[name].drop(columns=['temps_fins_reincidencia1a'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\pipilenes.py:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_copy[col].fillna(X_copy[col].mean(), inplace=True)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\pipilenes.py:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_copy[col].fillna(X_copy[col].mean(), inplace=True)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\pipilenes.py:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_copy[col].fillna(X_copy[col].mean(), inplace=True)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\pipilenes.py:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_copy[col].fillna(X_copy[col].mean(), inplace=True)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\pipilenes.py:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_copy[col].fillna(X_copy[col].mean(), inplace=True)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\pipilenes.py:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_copy[col].fillna(X_copy[col].mean(), inplace=True)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\pipilenes.py:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_copy[col].fillna(X_copy[col].mean(), inplace=True)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\pipilenes.py:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_copy[col].fillna(X_copy[col].mean(), inplace=True)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\pipilenes.py:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_copy[col].fillna(X_copy[col].mean(), inplace=True)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\pipilenes.py:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_copy[col].fillna(X_copy[col].mean(), inplace=True)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\pipilenes.py:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_copy[col].fillna(X_copy[col].mean(), inplace=True)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\pipilenes.py:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_copy[col].fillna(X_copy[col].mean(), inplace=True)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\pipilenes.py:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_copy[col].fillna(X_copy[col].mean(), inplace=True)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\pipilenes.py:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_copy[col].fillna(X_copy[col].mean(), inplace=True)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\pipilenes.py:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_copy[col].fillna(X_copy[col].mean(), inplace=True)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\pipilenes.py:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_copy[col].fillna(X_copy[col].mean(), inplace=True)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\pipilenes.py:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_copy[col].fillna(X_copy[col].mean(), inplace=True)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_test_splits = split_datasets_clas(datasets)  # Apliquem la separació\n",
    "\n",
    "transformed_train_sets = apply_pipeline_to_train_sets(train_test_splits)  # Apliquem la pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RAVEN01</th>\n",
       "      <th>RAVEN02</th>\n",
       "      <th>RAVEN03</th>\n",
       "      <th>RAVEN04</th>\n",
       "      <th>RAVEN05</th>\n",
       "      <th>RAVEN_PD</th>\n",
       "      <th>RAVEN_Pc</th>\n",
       "      <th>RAVEN_IQ</th>\n",
       "      <th>RAVENbarem</th>\n",
       "      <th>RavenZ</th>\n",
       "      <th>Raven_IQ2</th>\n",
       "      <th>RavenZ_2</th>\n",
       "      <th>Raven_IQ3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>11.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.019374</td>\n",
       "      <td>100.290611</td>\n",
       "      <td>-0.305331</td>\n",
       "      <td>95.420032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>12.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.470939</td>\n",
       "      <td>77.935917</td>\n",
       "      <td>-1.920840</td>\n",
       "      <td>71.187399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-2.067064</td>\n",
       "      <td>68.994039</td>\n",
       "      <td>-2.178248</td>\n",
       "      <td>67.326284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.129657</td>\n",
       "      <td>98.055142</td>\n",
       "      <td>-0.466882</td>\n",
       "      <td>92.996769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>11.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.874814</td>\n",
       "      <td>86.877794</td>\n",
       "      <td>-0.969789</td>\n",
       "      <td>85.453172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    RAVEN01  RAVEN02  RAVEN03  RAVEN04  RAVEN05  RAVEN_PD  RAVEN_Pc  RAVEN_IQ  \\\n",
       "14     11.0     10.0      8.0     10.0      7.0      46.0      50.0     100.0   \n",
       "38     12.0     11.0      2.0      6.0      5.0      36.0      10.0      80.0   \n",
       "31      7.0      7.0      5.0     10.0      3.0      32.0       5.0      76.0   \n",
       "61     12.0     10.0     10.0      9.0      4.0      45.0      50.0     100.0   \n",
       "48     11.0     10.0      7.0      9.0      3.0      40.0      25.0      90.0   \n",
       "\n",
       "    RAVENbarem    RavenZ   Raven_IQ2  RavenZ_2  Raven_IQ3  \n",
       "14         2.0  0.019374  100.290611 -0.305331  95.420032  \n",
       "38         2.0 -1.470939   77.935917 -1.920840  71.187399  \n",
       "31         2.0 -2.067064   68.994039 -2.178248  67.326284  \n",
       "61         2.0 -0.129657   98.055142 -0.466882  92.996769  \n",
       "48         2.0 -0.874814   86.877794 -0.969789  85.453172  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_train_sets[\"IQ\"]['X_train'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'datasets_separats' from 'c:\\\\Users\\\\jjf20\\\\OneDrive\\\\Documents\\\\Universitat\\\\TFG\\\\datasets_separats.py'>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets_separats\n",
    "import importlib\n",
    "importlib.reload(datasets_separats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrem a la funció:\n",
      "Applying significant features filter to dataset IQ...\n",
      "Tamanys train (57, 13)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 13)\n",
      "Tamanys train (57, 13)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 13)\n",
      "Applying significant features filter to dataset SAVRY...\n",
      "Tamanys train (57, 42)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 42)\n",
      "Tamanys train (57, 42)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 42)\n",
      "Applying significant features filter to dataset VAST...\n",
      "Tamanys train (57, 23)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 23)\n",
      "Tamanys train (57, 23)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 23)\n",
      "Applying significant features filter to dataset PCL...\n",
      "Tamanys train (57, 141)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 141)\n",
      "Tamanys train (57, 141)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 141)\n",
      "Applying significant features filter to dataset CAPE...\n",
      "Tamanys train (57, 17)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 17)\n",
      "Tamanys train (57, 17)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 17)\n",
      "Applying significant features filter to dataset YPI...\n",
      "Tamanys train (57, 75)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 75)\n",
      "Tamanys train (57, 75)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 75)\n",
      "Applying significant features filter to dataset RPQ...\n",
      "Tamanys train (57, 29)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 29)\n",
      "Tamanys train (57, 29)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 29)\n",
      "Applying significant features filter to dataset CCA...\n",
      "Tamanys train (57, 40)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 40)\n",
      "Tamanys train (57, 40)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 40)\n",
      "Applying significant features filter to dataset SD3...\n",
      "Tamanys train (57, 39)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 39)\n",
      "Tamanys train (57, 39)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 39)\n",
      "Applying significant features filter to dataset ICUJ...\n",
      "Tamanys train (57, 78)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 78)\n",
      "Tamanys train (57, 78)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 78)\n",
      "Applying significant features filter to dataset ICUT...\n",
      "Tamanys train (57, 30)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 30)\n",
      "Tamanys train (57, 30)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 30)\n",
      "Applying significant features filter to dataset TRFM...\n",
      "Tamanys train (57, 173)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 173)\n",
      "Tamanys train (57, 173)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 173)\n",
      "Applying significant features filter to dataset YSR...\n",
      "Tamanys train (57, 181)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 181)\n",
      "Tamanys train (57, 181)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 181)\n",
      "Applying significant features filter to dataset TriPM...\n",
      "Tamanys train (57, 87)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 87)\n",
      "Tamanys train (57, 87)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 87)\n",
      "Applying significant features filter to dataset TRFT...\n",
      "Tamanys train (57, 197)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 197)\n",
      "Tamanys train (57, 197)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 197)\n",
      "Applying significant features filter to dataset DD...\n",
      "Tamanys train (57, 24)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 24)\n",
      "Tamanys train (57, 24)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 24)\n"
     ]
    }
   ],
   "source": [
    "filtered_datasets = datasets_separats.apply_significant_features_filter_false(transformed_train_sets, df_orig)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m y_train_IQ \u001b[38;5;241m=\u001b[39m filtered_datasets[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIQ\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_train\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Executem el grid search\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_IQ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_IQ\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Resultats\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMillor paràmetre:\u001b[39m\u001b[38;5;124m\"\u001b[39m, grid_search\u001b[38;5;241m.\u001b[39mbest_params_)\n",
      "File \u001b[1;32mc:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1024\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1018\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1019\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1020\u001b[0m     )\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1024\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1027\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1028\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1571\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1570\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1571\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    963\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    964\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    965\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    966\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    967\u001b[0m         )\n\u001b[0;32m    968\u001b[0m     )\n\u001b[1;32m--> 970\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    976\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    981\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    982\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    984\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    985\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    986\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    989\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    990\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    991\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    992\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    993\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     76\u001b[0m )\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:139\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    137\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:893\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    891\u001b[0m     score_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time \u001b[38;5;241m-\u001b[39m fit_time\n\u001b[0;32m    892\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_train_score:\n\u001b[1;32m--> 893\u001b[0m         train_scores \u001b[38;5;241m=\u001b[39m \u001b[43m_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    894\u001b[0m \u001b[43m            \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscore_params_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_score\u001b[49m\n\u001b[0;32m    895\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    897\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    898\u001b[0m     total_time \u001b[38;5;241m=\u001b[39m score_time \u001b[38;5;241m+\u001b[39m fit_time\n",
      "File \u001b[1;32mc:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:949\u001b[0m, in \u001b[0;36m_score\u001b[1;34m(estimator, X_test, y_test, scorer, score_params, error_score)\u001b[0m\n\u001b[0;32m    947\u001b[0m         scores \u001b[38;5;241m=\u001b[39m scorer(estimator, X_test, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mscore_params)\n\u001b[0;32m    948\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 949\u001b[0m         scores \u001b[38;5;241m=\u001b[39m \u001b[43mscorer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mscore_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    950\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    951\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(scorer, _MultimetricScorer):\n\u001b[0;32m    952\u001b[0m         \u001b[38;5;66;03m# If `_MultimetricScorer` raises exception, the `error_score`\u001b[39;00m\n\u001b[0;32m    953\u001b[0m         \u001b[38;5;66;03m# parameter is equal to \"raise\".\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py:288\u001b[0m, in \u001b[0;36m_BaseScorer.__call__\u001b[1;34m(self, estimator, X, y_true, sample_weight, **kwargs)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    286\u001b[0m     _kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m sample_weight\n\u001b[1;32m--> 288\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_cached_call\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py:380\u001b[0m, in \u001b[0;36m_Scorer._score\u001b[1;34m(self, method_caller, estimator, X, y_true, **kwargs)\u001b[0m\n\u001b[0;32m    378\u001b[0m pos_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m is_regressor(estimator) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_pos_label()\n\u001b[0;32m    379\u001b[0m response_method \u001b[38;5;241m=\u001b[39m _check_response_method(estimator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_method)\n\u001b[1;32m--> 380\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmethod_caller\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_get_response_method_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_method\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    387\u001b[0m scoring_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sign \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_score_func(y_true, y_pred, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mscoring_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py:90\u001b[0m, in \u001b[0;36m_cached_call\u001b[1;34m(cache, estimator, response_method, *args, **kwargs)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m response_method \u001b[38;5;129;01min\u001b[39;00m cache:\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cache[response_method]\n\u001b[1;32m---> 90\u001b[0m result, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_get_response_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_method\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     cache[response_method] \u001b[38;5;241m=\u001b[39m result\n",
      "File \u001b[1;32mc:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\utils\\_response.py:214\u001b[0m, in \u001b[0;36m_get_response_values\u001b[1;34m(estimator, X, response_method, pos_label, return_response_method_used)\u001b[0m\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m pos_label \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m target_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    212\u001b[0m         pos_label \u001b[38;5;241m=\u001b[39m classes[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m--> 214\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mprediction_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prediction_method\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredict_proba\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredict_log_proba\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    217\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m _process_predict_proba(\n\u001b[0;32m    218\u001b[0m         y_pred\u001b[38;5;241m=\u001b[39my_pred,\n\u001b[0;32m    219\u001b[0m         target_type\u001b[38;5;241m=\u001b[39mtarget_type,\n\u001b[0;32m    220\u001b[0m         classes\u001b[38;5;241m=\u001b[39mclasses,\n\u001b[0;32m    221\u001b[0m         pos_label\u001b[38;5;241m=\u001b[39mpos_label,\n\u001b[0;32m    222\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:904\u001b[0m, in \u001b[0;36mForestClassifier.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    884\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    885\u001b[0m \u001b[38;5;124;03m    Predict class for X.\u001b[39;00m\n\u001b[0;32m    886\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;124;03m        The predicted classes.\u001b[39;00m\n\u001b[0;32m    903\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 904\u001b[0m     proba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    907\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39mtake(np\u001b[38;5;241m.\u001b[39margmax(proba, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:957\u001b[0m, in \u001b[0;36mForestClassifier.predict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    952\u001b[0m all_proba \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    953\u001b[0m     np\u001b[38;5;241m.\u001b[39mzeros((X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], j), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m    954\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39matleast_1d(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_)\n\u001b[0;32m    955\u001b[0m ]\n\u001b[0;32m    956\u001b[0m lock \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mLock()\n\u001b[1;32m--> 957\u001b[0m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequire\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msharedmem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_accumulate_prediction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_proba\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlock\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimators_\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m proba \u001b[38;5;129;01min\u001b[39;00m all_proba:\n\u001b[0;32m    963\u001b[0m     proba \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_)\n",
      "File \u001b[1;32mc:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     76\u001b[0m )\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:139\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    137\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:731\u001b[0m, in \u001b[0;36m_accumulate_prediction\u001b[1;34m(predict, X, out, lock)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_accumulate_prediction\u001b[39m(predict, X, out, lock):\n\u001b[0;32m    725\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    726\u001b[0m \u001b[38;5;124;03m    This is a utility function for joblib's Parallel.\u001b[39;00m\n\u001b[0;32m    727\u001b[0m \n\u001b[0;32m    728\u001b[0m \u001b[38;5;124;03m    It can't go locally in ForestClassifier or ForestRegressor, because joblib\u001b[39;00m\n\u001b[0;32m    729\u001b[0m \u001b[38;5;124;03m    complains that it cannot pickle it when placed there.\u001b[39;00m\n\u001b[0;32m    730\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 731\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m lock:\n\u001b[0;32m    733\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\tree\\_classes.py:1058\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.predict_proba\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m   1056\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1057\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_X_predict(X, check_input)\n\u001b[1;32m-> 1058\u001b[0m proba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1060\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1061\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m proba[:, : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# Definim la graella d'hiperparàmetres\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 250, 500],\n",
    "    'max_depth': [10,15,30],\n",
    "}\n",
    "\n",
    "# Model de classificació\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# GridSearch amb cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    rf_classifier,\n",
    "    param_grid,\n",
    "    cv=10,\n",
    "    scoring='accuracy',  # Pots canviar-ho per 'f1', 'roc_auc', etc.\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Dades d'entrenament (hauries de tenir les columnes correctament preparades)\n",
    "X_train_IQ = filtered_datasets['IQ']['X_train_filtrat']\n",
    "y_train_IQ = filtered_datasets['IQ']['y_train']\n",
    "\n",
    "# Executem el grid search\n",
    "grid_search.fit(X_train_IQ, y_train_IQ)\n",
    "\n",
    "# Resultats\n",
    "print(\"Millor paràmetre:\", grid_search.best_params_)\n",
    "print(\"Millor accuracy mitjana en CV:\", grid_search.best_score_)\n",
    "\n",
    "# Mostrem la mitjana general de test\n",
    "print(\"Mitjana dels resultats de test per cada combinació:\")\n",
    "print(np.mean(grid_search.cv_results_['mean_test_score']))\n",
    "\n",
    "# Model final amb millors hiperparàmetres\n",
    "best_model = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les 20 característiques més importants són:\n",
      "Raven_IQ3: 0.1360\n",
      "RavenZ_2: 0.1265\n",
      "RAVEN05: 0.0889\n",
      "RAVEN02: 0.0885\n",
      "RavenZ: 0.0816\n",
      "Raven_IQ2: 0.0813\n",
      "RAVEN01: 0.0754\n",
      "RAVEN_PD: 0.0749\n",
      "RAVEN04: 0.0734\n",
      "RAVEN03: 0.0617\n",
      "RAVEN_IQ: 0.0417\n",
      "RAVEN_Pc: 0.0366\n",
      "RAVENbarem: 0.0335\n"
     ]
    }
   ],
   "source": [
    "# Obtenim la importància de cada característica\n",
    "coef_importants = best_model.feature_importances_\n",
    "\n",
    "# Obtenim els noms de les característiques (suposant que tenim un DataFrame 'df')\n",
    "feature_names = X_train_IQ.columns  \n",
    "\n",
    "# Ordenem les característiques segons la seva importància (de major a menor)\n",
    "indices_ordenats = coef_importants.argsort()[::-1][0:25]\n",
    "\n",
    "# Mostrem els noms de les 20 característiques més importants amb la seva importància\n",
    "print(\"Les 20 característiques més importants són:\")\n",
    "for i in indices_ordenats:\n",
    "    print(f\"{feature_names[i]}: {coef_importants[i]:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les millors característiques son RAVEN05 i RAVEN02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ara comencem amb el segon dataset que és SAVRY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Millor paràmetre: {'max_depth': 10, 'n_estimators': 250}\n",
      "Millor accuracy: 0.6166666666666667\n",
      "Mitjana dels resultats de test per cada combinació:\n",
      "0.5977777777777777\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# Definim la graella d'hiperparàmetres\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 250, 500],\n",
    "    'max_depth': [10, 20, 30],\n",
    "}\n",
    "\n",
    "# Model de classificació\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    rf, \n",
    "    param_grid, \n",
    "    cv=10, \n",
    "    scoring='accuracy',  # Pots canviar a 'f1', 'roc_auc', etc.\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "X_train_SAVRY = filtered_datasets['SAVRY']['X_train_filtrat']\n",
    "y_train_SAVRY = filtered_datasets['SAVRY']['y_train']\n",
    "\n",
    "# Realitzem el Grid Search\n",
    "grid_search.fit(X_train_SAVRY, y_train_SAVRY)\n",
    "\n",
    "# Resultats\n",
    "print(\"Millor paràmetre:\", grid_search.best_params_)\n",
    "print(\"Millor accuracy:\", grid_search.best_score_)\n",
    "\n",
    "# Mostrem la mitjana dels resultats de test\n",
    "print(\"Mitjana dels resultats de test per cada combinació:\")\n",
    "print(np.mean(grid_search.cv_results_['mean_test_score']))\n",
    "\n",
    "best_model = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les 20 característiques més importants són:\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 41 is out of bounds for axis 0 with size 13",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLes 20 característiques més importants són:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m indices_ordenats:\n\u001b[1;32m---> 13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mfeature_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcoef_importants[i]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:5389\u001b[0m, in \u001b[0;36mIndex.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   5386\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(key) \u001b[38;5;129;01mor\u001b[39;00m is_float(key):\n\u001b[0;32m   5387\u001b[0m     \u001b[38;5;66;03m# GH#44051 exclude bool, which would return a 2d ndarray\u001b[39;00m\n\u001b[0;32m   5388\u001b[0m     key \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mcast_scalar_indexer(key)\n\u001b[1;32m-> 5389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgetitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mslice\u001b[39m):\n\u001b[0;32m   5392\u001b[0m     \u001b[38;5;66;03m# This case is separated from the conditional above to avoid\u001b[39;00m\n\u001b[0;32m   5393\u001b[0m     \u001b[38;5;66;03m# pessimization com.is_bool_indexer and ndim checks.\u001b[39;00m\n\u001b[0;32m   5394\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_slice(key)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 41 is out of bounds for axis 0 with size 13"
     ]
    }
   ],
   "source": [
    "# Obtenim la importància de cada característica\n",
    "coef_importants = best_model.feature_importances_\n",
    "\n",
    "# Obtenim els noms de les característiques (suposant que tenim un DataFrame 'df')\n",
    "feature_names = X_train_IQ.columns  \n",
    "\n",
    "# Ordenem les característiques segons la seva importància (de major a menor)\n",
    "indices_ordenats = coef_importants.argsort()[::-1][0:25]\n",
    "\n",
    "# Mostrem els noms de les 20 característiques més importants amb la seva importància\n",
    "print(\"Les 20 característiques més importants són:\")\n",
    "for i in indices_ordenats:\n",
    "    print(f\"{feature_names[i]}: {coef_importants[i]:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les millos caractqristiques\n",
    "SAVRYHis: 0.1155\n",
    "SAVRY05: 0.1019\n",
    "SAVRYTot: 0.0970"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fem ara el model DD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Millor paràmetre: {'max_depth': 10, 'n_estimators': 250}\n",
      "Millor accuracy: 0.4833333333333333\n",
      "Mitjana dels resultats de test per cada combinació:\n",
      "0.47111111111111104\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# Definim la graella d'hiperparàmetres\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 250, 500],\n",
    "    'max_depth': [10, 20, 30],\n",
    "}\n",
    "\n",
    "# Model de classificació\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    rf, \n",
    "    param_grid, \n",
    "    cv=10, \n",
    "    scoring='accuracy',  # Pots substituir per 'f1', 'roc_auc', etc.\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Dades del subdataset DD\n",
    "X_train_DD = filtered_datasets['DD']['X_train_filtrat']\n",
    "y_train_DD = filtered_datasets['DD']['y_train']\n",
    "\n",
    "# Realitzem el Grid Search\n",
    "grid_search.fit(X_train_DD, y_train_DD)\n",
    "\n",
    "# Resultats\n",
    "print(\"Millor paràmetre:\", grid_search.best_params_)\n",
    "print(\"Millor accuracy:\", grid_search.best_score_)\n",
    "\n",
    "# Mostrem la mitjana dels resultats de test\n",
    "print(\"Mitjana dels resultats de test per cada combinació:\")\n",
    "print(np.mean(grid_search.cv_results_['mean_test_score']))\n",
    "\n",
    "best_model = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les 20 característiques més importants són:\n",
      "DD07: 0.1034\n",
      "DDNarc: 0.0914\n",
      "DDPsyc: 0.0848\n",
      "DDTotal: 0.0773\n",
      "DD02: 0.0686\n",
      "DD03: 0.0488\n",
      "DD11: 0.0479\n",
      "DD12: 0.0455\n",
      "DD06: 0.0441\n",
      "DD05: 0.0441\n",
      "DDMach: 0.0432\n",
      "DD10: 0.0420\n",
      "DD08: 0.0391\n",
      "DD09: 0.0387\n",
      "DD01: 0.0358\n",
      "DD04: 0.0285\n",
      "DDPsyc_3gr: 0.0259\n",
      "DDTotal_3gr: 0.0212\n",
      "DDNarc_3gr: 0.0163\n",
      "DDMach_3gr: 0.0129\n",
      "DDPsyc_2gr: 0.0129\n",
      "DDNarc_2gr: 0.0098\n",
      "DDTotal_2gr: 0.0090\n",
      "DDMach_2gr: 0.0089\n"
     ]
    }
   ],
   "source": [
    "# Obtenim la importància de cada característica\n",
    "coef_importants = best_model.feature_importances_\n",
    "\n",
    "# Obtenim els noms de les característiques (suposant que tenim un DataFrame 'df')\n",
    "feature_names = X_train_DD.columns  \n",
    "\n",
    "# Ordenem les característiques segons la seva importància (de major a menor)\n",
    "indices_ordenats = coef_importants.argsort()[::-1][0:25]\n",
    "\n",
    "# Mostrem els noms de les 20 característiques més importants amb la seva importància\n",
    "print(\"Les 20 característiques més importants són:\")\n",
    "for i in indices_ordenats:\n",
    "    print(f\"{feature_names[i]}: {coef_importants[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ara farem VAST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Millor paràmetre: {'max_depth': 15, 'n_estimators': 500}\n",
      "Millor accuracy: 0.4766666666666667\n",
      "Mitjana dels resultats de test per cada combinació:\n",
      "0.4655555555555555\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# Definim la graella d'hiperparàmetres\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 250, 500],\n",
    "    'max_depth': [15, 30, 45],\n",
    "}\n",
    "\n",
    "# Model de classificació\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    rf, \n",
    "    param_grid, \n",
    "    cv=10, \n",
    "    scoring='accuracy', \n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Dades del subdataset VAST\n",
    "X_train_VAST = filtered_datasets['VAST']['X_train_filtrat']\n",
    "y_train_VAST = filtered_datasets['VAST']['y_train']\n",
    "\n",
    "# Realitzem el Grid Search\n",
    "grid_search.fit(X_train_VAST, y_train_VAST)\n",
    "\n",
    "# Resultats\n",
    "print(\"Millor paràmetre:\", grid_search.best_params_)\n",
    "print(\"Millor accuracy:\", grid_search.best_score_)\n",
    "\n",
    "# Mostrem la mitjana dels resultats de test\n",
    "print(\"Mitjana dels resultats de test per cada combinació:\")\n",
    "print(np.mean(grid_search.cv_results_['mean_test_score']))\n",
    "\n",
    "# Model amb millors hiperparàmetres\n",
    "best_model = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les 20 característiques més importants són:\n",
      "VASTTotal: 0.0763\n",
      "VASTVic: 0.0717\n",
      "VAST02: 0.0695\n",
      "VASTDir: 0.0621\n",
      "VAST14: 0.0508\n",
      "VAST16: 0.0494\n",
      "VAST12: 0.0481\n",
      "VAST15: 0.0475\n",
      "VAST07: 0.0469\n",
      "VAST09: 0.0469\n",
      "VAST11: 0.0458\n",
      "VAST17: 0.0410\n",
      "VAST05: 0.0394\n",
      "VAST06: 0.0380\n",
      "VAST08: 0.0378\n",
      "VAST10: 0.0369\n",
      "VAST19: 0.0349\n",
      "VAST18: 0.0312\n",
      "VAST03: 0.0306\n",
      "VAST20: 0.0305\n",
      "VAST01: 0.0243\n",
      "VAST04: 0.0228\n",
      "VAST13: 0.0176\n"
     ]
    }
   ],
   "source": [
    "# Obtenim la importància de cada característica\n",
    "coef_importants = best_model.feature_importances_\n",
    "\n",
    "# Obtenim els noms de les característiques (suposant que tenim un DataFrame 'df')\n",
    "feature_names = X_train_VAST.columns  \n",
    "\n",
    "# Ordenem les característiques segons la seva importància (de major a menor)\n",
    "indices_ordenats = coef_importants.argsort()[::-1][0:25]\n",
    "\n",
    "# Mostrem els noms de les 20 característiques més importants amb la seva importància\n",
    "print(\"Les 20 característiques més importants són:\")\n",
    "for i in indices_ordenats:\n",
    "    print(f\"{feature_names[i]}: {coef_importants[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fem el dataset de PCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Millor paràmetre: {'max_depth': 30, 'n_estimators': 100}\n",
      "Millor accuracy: 0.51\n",
      "Mitjana dels resultats de test per cada combinació:\n",
      "0.5077777777777777\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# Graella d'hiperparàmetres\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 250, 500],\n",
    "    'max_depth': [30, 50, 75],\n",
    "}\n",
    "\n",
    "# Model de classificació\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Grid Search amb cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    rf, \n",
    "    param_grid, \n",
    "    cv=10, \n",
    "    scoring='accuracy', \n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Dades del subdataset PCL\n",
    "X_train_PCL = filtered_datasets['PCL']['X_train_filtrat']\n",
    "y_train_PCL = filtered_datasets['PCL']['y_train']\n",
    "\n",
    "# Entrenem el model\n",
    "grid_search.fit(X_train_PCL, y_train_PCL)\n",
    "\n",
    "# Resultats\n",
    "print(\"Millor paràmetre:\", grid_search.best_params_)\n",
    "print(\"Millor accuracy:\", grid_search.best_score_)\n",
    "\n",
    "# Mitjana general dels resultats de test\n",
    "print(\"Mitjana dels resultats de test per cada combinació:\")\n",
    "print(np.mean(grid_search.cv_results_['mean_test_score']))\n",
    "\n",
    "# Model òptim obtingut\n",
    "best_model = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les 20 característiques més importants són:\n",
      "PCLj07: 0.0210\n",
      "PCLYVM13: 0.0196\n",
      "PCLeFAC_3_2gr: 0.0194\n",
      "PCLYVMFAC_1: 0.0178\n",
      "PCLo14: 0.0175\n",
      "PCLYVMFAC_3: 0.0175\n",
      "PCLeFAC_3: 0.0174\n",
      "PCLYVM14: 0.0173\n",
      "PCLe14: 0.0165\n",
      "PCLe_PT: 0.0163\n",
      "PCLYVM02: 0.0162\n",
      "PCLe13: 0.0156\n",
      "PCLe20: 0.0154\n",
      "PCLe10: 0.0149\n",
      "PCLo_PT: 0.0146\n",
      "PCLYVM16: 0.0146\n",
      "PCLYVM11: 0.0145\n",
      "PCLYVMFAC_2: 0.0143\n",
      "PCLoFAC_3: 0.0141\n",
      "PCLeFAC_4: 0.0134\n",
      "PCLe03: 0.0133\n",
      "PCLe19: 0.0127\n",
      "PCLoFAC_4: 0.0125\n",
      "PCLYVMPT: 0.0123\n",
      "PCLxFAC_4: 0.0123\n"
     ]
    }
   ],
   "source": [
    "# Obtenim la importància de cada característica\n",
    "coef_importants = best_model.feature_importances_\n",
    "\n",
    "# Obtenim els noms de les característiques (suposant que tenim un DataFrame 'df')\n",
    "feature_names = X_train_PCL.columns  \n",
    "\n",
    "# Ordenem les característiques segons la seva importància (de major a menor)\n",
    "indices_ordenats = coef_importants.argsort()[::-1][0:25]\n",
    "\n",
    "# Mostrem els noms de les 20 característiques més importants amb la seva importància\n",
    "print(\"Les 20 característiques més importants són:\")\n",
    "for i in indices_ordenats:\n",
    "    print(f\"{feature_names[i]}: {coef_importants[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claus amb valor PCLjFAC_3: [258]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "valor_que_busco = \"PCLjFAC_3\"\n",
    "\n",
    "claus = [clau for clau, valor in dict.items() if valor == valor_que_busco]\n",
    "\n",
    "print(f\"Claus amb valor {valor_que_busco}: {claus}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ara anem a fer CAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Millor paràmetre: {'max_depth': 30, 'n_estimators': 100}\n",
      "Millor accuracy: 0.5166666666666667\n",
      "Mitjana dels resultats de test per cada combinació:\n",
      "0.4911111111111111\n",
      "Les 25 característiques més importants són:\n",
      "CAPEo03: 0.1310\n",
      "CAPEe_PT_3items: 0.0985\n",
      "CAPEo04: 0.0730\n",
      "CAPEe03: 0.0725\n",
      "CAPEo01: 0.0685\n",
      "CAPEo201: 0.0679\n",
      "CAPEo202: 0.0673\n",
      "CAPEe02: 0.0660\n",
      "CAPEo203: 0.0621\n",
      "CAPEo204: 0.0579\n",
      "CAPEo02: 0.0510\n",
      "CAPEe04: 0.0409\n",
      "CAPEe_PT_3gr: 0.0379\n",
      "CAPEo_PT_3gr: 0.0367\n",
      "CAPEe01: 0.0350\n",
      "CAPEe_PT_2gr: 0.0184\n",
      "CAPEo_PT_2gr: 0.0153\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# Definim la graella d'hiperparàmetres\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 250, 500],\n",
    "    'max_depth': [30, 50, 75],\n",
    "}\n",
    "\n",
    "# Model de classificació\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Grid Search amb cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    rf, \n",
    "    param_grid, \n",
    "    cv=10, \n",
    "    scoring='accuracy', \n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Dades d'entrenament\n",
    "X_train_CAPE = filtered_datasets['CAPE']['X_train_filtrat']\n",
    "y_train_CAPE = filtered_datasets['CAPE']['y_train']\n",
    "\n",
    "# Entrenem el model\n",
    "grid_search.fit(X_train_CAPE, y_train_CAPE)\n",
    "\n",
    "# Resultats del Grid Search\n",
    "print(\"Millor paràmetre:\", grid_search.best_params_)\n",
    "print(\"Millor accuracy:\", grid_search.best_score_)\n",
    "\n",
    "print(\"Mitjana dels resultats de test per cada combinació:\")\n",
    "print(np.mean(grid_search.cv_results_['mean_test_score']))\n",
    "\n",
    "# Millor model obtingut\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Importància de característiques\n",
    "coef_importants = best_model.feature_importances_\n",
    "feature_names = X_train_CAPE.columns  \n",
    "\n",
    "# Ordenem les 25 característiques més rellevants\n",
    "indices_ordenats = coef_importants.argsort()[::-1][:25]\n",
    "\n",
    "print(\"Les 25 característiques més importants són:\")\n",
    "for i in indices_ordenats:\n",
    "    print(f\"{feature_names[i]}: {coef_importants[i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ara anem a fer YPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Millor paràmetre: {'max_depth': 20, 'n_estimators': 500}\n",
      "Millor accuracy: 0.51\n",
      "Mitjana dels resultats de test per cada combinació:\n",
      "0.48777777777777775\n",
      "Les 25 característiques més importants són:\n",
      "YPI21: 0.0414\n",
      "YPI_RE: 0.0331\n",
      "YPI35: 0.0267\n",
      "YPI_II: 0.0259\n",
      "YPI_CU: 0.0251\n",
      "YPI_TS: 0.0226\n",
      "YPI_PT: 0.0225\n",
      "YPI_IM: 0.0212\n",
      "YPI_GR: 0.0204\n",
      "YPI09: 0.0196\n",
      "xYPI35: 0.0195\n",
      "YPI_IR: 0.0191\n",
      "YPI_GM: 0.0191\n",
      "YPI_CA: 0.0184\n",
      "YPI_MA: 0.0179\n",
      "YPI31: 0.0177\n",
      "YPI08: 0.0174\n",
      "YPI_UN: 0.0168\n",
      "YPI18: 0.0168\n",
      "YPI32: 0.0165\n",
      "YPI03: 0.0162\n",
      "YPI11: 0.0160\n",
      "YPI_LY: 0.0155\n",
      "YPI41: 0.0152\n",
      "YPI_DC: 0.0152\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# Definim la graella d'hiperparàmetres\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 250, 500],\n",
    "    'max_depth': [20, 30, 50],\n",
    "}\n",
    "\n",
    "# Model de classificació\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Configuració del grid search\n",
    "grid_search = GridSearchCV(\n",
    "    rf, \n",
    "    param_grid, \n",
    "    cv=10, \n",
    "    scoring='accuracy', \n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Dades del subdataset YPI\n",
    "X_train_YPI = filtered_datasets['YPI']['X_train_filtrat']\n",
    "y_train_YPI = filtered_datasets['YPI']['y_train']\n",
    "\n",
    "# Entrenament\n",
    "grid_search.fit(X_train_YPI, y_train_YPI)\n",
    "\n",
    "# Resultats del grid search\n",
    "print(\"Millor paràmetre:\", grid_search.best_params_)\n",
    "print(\"Millor accuracy:\", grid_search.best_score_)\n",
    "\n",
    "print(\"Mitjana dels resultats de test per cada combinació:\")\n",
    "print(np.mean(grid_search.cv_results_['mean_test_score']))\n",
    "\n",
    "# Millor model entrenat\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Importància de característiques\n",
    "coef_importants = best_model.feature_importances_\n",
    "feature_names = X_train_YPI.columns  \n",
    "\n",
    "# Ordenem i mostrem les 25 més importants\n",
    "indices_ordenats = coef_importants.argsort()[::-1][:25]\n",
    "\n",
    "print(\"Les 25 característiques més importants són:\")\n",
    "for i in indices_ordenats:\n",
    "    print(f\"{feature_names[i]}: {coef_importants[i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Farem servir servir el dataset RPQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Millor paràmetre: {'max_depth': 30, 'n_estimators': 500}\n",
      "Millor accuracy: 0.5166666666666666\n",
      "Mitjana dels resultats de test per cada combinació:\n",
      "0.5055555555555555\n",
      "Les 25 característiques més importants són:\n",
      "RPQ_PT: 0.0943\n",
      "RPQ_RA: 0.0888\n",
      "RPQ_PA: 0.0835\n",
      "RPQ18: 0.0478\n",
      "RPQ07: 0.0468\n",
      "RPQ14: 0.0421\n",
      "RPQ19: 0.0354\n",
      "RPQ20: 0.0351\n",
      "RPQ09: 0.0342\n",
      "RPQ22: 0.0333\n",
      "RPQ21: 0.0329\n",
      "RPQ10: 0.0328\n",
      "RPQ06: 0.0326\n",
      "RPQ13: 0.0323\n",
      "RPQ12: 0.0284\n",
      "RPQ02: 0.0261\n",
      "RPQ_PA_50: 0.0260\n",
      "RPQ16: 0.0257\n",
      "RPQ08: 0.0249\n",
      "RPQ17: 0.0241\n",
      "RPQ03: 0.0234\n",
      "RPQ15: 0.0220\n",
      "RPQ11: 0.0211\n",
      "RPQ04: 0.0200\n",
      "RPQ05: 0.0198\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# Definim la graella d'hiperparàmetres\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 250, 500],\n",
    "    'max_depth': [30, 50, 75],\n",
    "}\n",
    "\n",
    "# Model de classificació\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Configuració del grid search\n",
    "grid_search = GridSearchCV(\n",
    "    rf, \n",
    "    param_grid, \n",
    "    cv=10, \n",
    "    scoring='accuracy', \n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Dades d'entrenament del subdataset RPQ\n",
    "X_train_RPQ = filtered_datasets['RPQ']['X_train_filtrat']\n",
    "y_train_RPQ = filtered_datasets['RPQ']['y_train']\n",
    "\n",
    "# Entrenem el model\n",
    "grid_search.fit(X_train_RPQ, y_train_RPQ)\n",
    "\n",
    "# Resultats\n",
    "print(\"Millor paràmetre:\", grid_search.best_params_)\n",
    "print(\"Millor accuracy:\", grid_search.best_score_)\n",
    "\n",
    "print(\"Mitjana dels resultats de test per cada combinació:\")\n",
    "print(np.mean(grid_search.cv_results_['mean_test_score']))\n",
    "\n",
    "# Millor model final\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Importància de característiques\n",
    "coef_importants = best_model.feature_importances_\n",
    "feature_names = X_train_RPQ.columns  \n",
    "\n",
    "# Top 25 característiques\n",
    "indices_ordenats = coef_importants.argsort()[::-1][:25]\n",
    "\n",
    "print(\"Les 25 característiques més importants són:\")\n",
    "for i in indices_ordenats:\n",
    "    print(f\"{feature_names[i]}: {coef_importants[i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ara farem CCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Millor paràmetre: {'max_depth': 30, 'n_estimators': 100}\n",
      "Millor accuracy: 0.32999999999999996\n",
      "Mitjana dels resultats de test per cada combinació:\n",
      "0.32999999999999996\n",
      "Les 25 característiques més importants són:\n",
      "CCA_CN: 0.0621\n",
      "CCA_DR: 0.0517\n",
      "CCA_RO: 0.0515\n",
      "CCA_PT: 0.0514\n",
      "CCA_AG: 0.0487\n",
      "CCA_VA: 0.0396\n",
      "CCA21: 0.0377\n",
      "CCA06: 0.0328\n",
      "CCA08: 0.0311\n",
      "CCA14: 0.0285\n",
      "CCA11: 0.0281\n",
      "CCA15: 0.0276\n",
      "CCA18: 0.0260\n",
      "CCA31: 0.0254\n",
      "CCA_CO: 0.0253\n",
      "CCA07: 0.0248\n",
      "CCA04: 0.0234\n",
      "CCA22: 0.0233\n",
      "CCA32: 0.0229\n",
      "CCA26: 0.0228\n",
      "CCA19: 0.0225\n",
      "CCA02: 0.0210\n",
      "CCA29: 0.0206\n",
      "CCA24: 0.0204\n",
      "CCA20: 0.0187\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# Definim la graella d'hiperparàmetres\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 250, 500],\n",
    "    'max_depth': [30, 50, 75],\n",
    "}\n",
    "\n",
    "# Model de classificació\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Configuració del grid search\n",
    "grid_search = GridSearchCV(\n",
    "    rf, \n",
    "    param_grid, \n",
    "    cv=10, \n",
    "    scoring='accuracy', \n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Dades del subdataset CCA\n",
    "X_train_CCA = filtered_datasets['CCA']['X_train_filtrat']\n",
    "y_train_CCA = filtered_datasets['CCA']['y_train']\n",
    "\n",
    "# Entrenament\n",
    "grid_search.fit(X_train_CCA, y_train_CCA)\n",
    "\n",
    "# Resultats\n",
    "print(\"Millor paràmetre:\", grid_search.best_params_)\n",
    "print(\"Millor accuracy:\", grid_search.best_score_)\n",
    "\n",
    "print(\"Mitjana dels resultats de test per cada combinació:\")\n",
    "print(np.mean(grid_search.cv_results_['mean_test_score']))\n",
    "\n",
    "# Millor model obtingut\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Importància de característiques\n",
    "coef_importants = best_model.feature_importances_\n",
    "feature_names = X_train_CCA.columns  \n",
    "\n",
    "# Top 25 característiques més importants\n",
    "indices_ordenats = coef_importants.argsort()[::-1][:25]\n",
    "\n",
    "print(\"Les 25 característiques més importants són:\")\n",
    "for i in indices_ordenats:\n",
    "    print(f\"{feature_names[i]}: {coef_importants[i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Farem el SD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Millor paràmetre: {'max_depth': 30, 'n_estimators': 500}\n",
      "Millor accuracy: 0.53\n",
      "Mitjana dels resultats de test per cada combinació:\n",
      "0.5077777777777777\n",
      "Les 25 característiques més importants són:\n",
      "xSD315: 0.0533\n",
      "SD3Narc: 0.0477\n",
      "SD3Mach: 0.0466\n",
      "SD326: 0.0427\n",
      "SD3Psyc: 0.0396\n",
      "SD315: 0.0374\n",
      "SD309: 0.0343\n",
      "SD302: 0.0330\n",
      "SD303: 0.0324\n",
      "SD307: 0.0310\n",
      "SD301: 0.0299\n",
      "SD305: 0.0297\n",
      "SD314: 0.0294\n",
      "SD318: 0.0288\n",
      "SD322: 0.0286\n",
      "SD310: 0.0278\n",
      "SD320: 0.0273\n",
      "SD316: 0.0261\n",
      "SD319: 0.0253\n",
      "xSD311: 0.0236\n",
      "xSD320: 0.0234\n",
      "SD311: 0.0230\n",
      "SD306: 0.0228\n",
      "SD327: 0.0225\n",
      "SD312: 0.0217\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# Graella d'hiperparàmetres\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 250, 500],\n",
    "    'max_depth': [30, 50, 75],\n",
    "}\n",
    "\n",
    "# Model de classificació\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Configuració del grid search\n",
    "grid_search = GridSearchCV(\n",
    "    rf, \n",
    "    param_grid, \n",
    "    cv=10, \n",
    "    scoring='accuracy', \n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Dades del subdataset SD3\n",
    "X_train_SD3 = filtered_datasets['SD3']['X_train_filtrat']\n",
    "y_train_SD3 = filtered_datasets['SD3']['y_train']\n",
    "\n",
    "# Entrenem el model\n",
    "grid_search.fit(X_train_SD3, y_train_SD3)\n",
    "\n",
    "# Resultats\n",
    "print(\"Millor paràmetre:\", grid_search.best_params_)\n",
    "print(\"Millor accuracy:\", grid_search.best_score_)\n",
    "\n",
    "print(\"Mitjana dels resultats de test per cada combinació:\")\n",
    "print(np.mean(grid_search.cv_results_['mean_test_score']))\n",
    "\n",
    "# Model final\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Importància de característiques\n",
    "coef_importants = best_model.feature_importances_\n",
    "feature_names = X_train_SD3.columns  \n",
    "\n",
    "# Top 25 característiques més rellevants\n",
    "indices_ordenats = coef_importants.argsort()[::-1][:25]\n",
    "\n",
    "print(\"Les 25 característiques més importants són:\")\n",
    "for i in indices_ordenats:\n",
    "    print(f\"{feature_names[i]}: {coef_importants[i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Farem el ICUJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Millor paràmetre: {'max_depth': 30, 'n_estimators': 100}\n",
      "Millor accuracy: 0.59\n",
      "Mitjana dels resultats de test per cada combinació:\n",
      "0.5233333333333332\n",
      "Les 25 característiques més importants són:\n",
      "xICUT14: 0.0543\n",
      "xICUT23: 0.0433\n",
      "ICUJ_PT: 0.0366\n",
      "ICUJ12: 0.0346\n",
      "ICUJ_CA: 0.0299\n",
      "ICUJsp4ExtrmN: 0.0299\n",
      "ICUJ14: 0.0281\n",
      "ICUTsp4SplitN: 0.0270\n",
      "ICUJ_UE: 0.0247\n",
      "ICUTsp9ExtrmN: 0.0233\n",
      "xICUT19: 0.0225\n",
      "xICUJ14: 0.0198\n",
      "xICUT15: 0.0197\n",
      "ICUJ20: 0.0193\n",
      "ICUJ18: 0.0188\n",
      "xICUT24: 0.0179\n",
      "ICUTsp8Extrm: 0.0174\n",
      "ICUJ02: 0.0174\n",
      "xICUT01: 0.0173\n",
      "ICUJ_UC: 0.0172\n",
      "ICUJ19: 0.0161\n",
      "xICUJ23: 0.0156\n",
      "ICUJ07: 0.0148\n",
      "ICUJsp8SplitN: 0.0145\n",
      "xICUJ24: 0.0145\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# Graella d'hiperparàmetres\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 250, 500],\n",
    "    'max_depth': [30, 50, 75],\n",
    "}\n",
    "\n",
    "# Model de classificació\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Configuració del grid search\n",
    "grid_search = GridSearchCV(\n",
    "    rf, \n",
    "    param_grid, \n",
    "    cv=10, \n",
    "    scoring='accuracy', \n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Dades del subdataset ICUJ\n",
    "X_train_ICUJ = filtered_datasets['ICUJ']['X_train_filtrat']\n",
    "y_train_ICUJ = filtered_datasets['ICUJ']['y_train']\n",
    "\n",
    "# Entrenem el model\n",
    "grid_search.fit(X_train_ICUJ, y_train_ICUJ)\n",
    "\n",
    "# Resultats\n",
    "print(\"Millor paràmetre:\", grid_search.best_params_)\n",
    "print(\"Millor accuracy:\", grid_search.best_score_)\n",
    "\n",
    "print(\"Mitjana dels resultats de test per cada combinació:\")\n",
    "print(np.mean(grid_search.cv_results_['mean_test_score']))\n",
    "\n",
    "# Model final\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Importància de característiques\n",
    "coef_importants = best_model.feature_importances_\n",
    "feature_names = X_train_ICUJ.columns  \n",
    "\n",
    "# Top 25 característiques més rellevants\n",
    "indices_ordenats = coef_importants.argsort()[::-1][:25]\n",
    "\n",
    "print(\"Les 25 característiques més importants són:\")\n",
    "for i in indices_ordenats:\n",
    "    print(f\"{feature_names[i]}: {coef_importants[i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Farem el ICUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Millor paràmetre: {'max_depth': 30, 'n_estimators': 100}\n",
      "Millor accuracy: 0.6233333333333333\n",
      "Mitjana dels resultats de test per cada combinació:\n",
      "0.591111111111111\n",
      "Les 25 característiques més importants són:\n",
      "ICUT_PT: 0.0792\n",
      "ICUT_UE: 0.0684\n",
      "ICUT14: 0.0668\n",
      "ICUT15: 0.0567\n",
      "ICUT_CA: 0.0536\n",
      "ICUT23: 0.0490\n",
      "ICUT11: 0.0443\n",
      "ICUT19: 0.0394\n",
      "ICUT01: 0.0376\n",
      "ICUT_UC: 0.0369\n",
      "ICUT05: 0.0350\n",
      "ICUT22: 0.0339\n",
      "ICUT03: 0.0314\n",
      "ICUT10: 0.0309\n",
      "ICUT21: 0.0296\n",
      "ICUT09: 0.0293\n",
      "ICUT24: 0.0286\n",
      "ICUT13: 0.0273\n",
      "ICUT06: 0.0264\n",
      "ICUT08: 0.0238\n",
      "ICUT16: 0.0233\n",
      "ICUT18: 0.0213\n",
      "ICUT07: 0.0213\n",
      "ICUT04: 0.0182\n",
      "ICUT02: 0.0180\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# Graella d'hiperparàmetres\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 250, 500],\n",
    "    'max_depth': [30, 50, 75],\n",
    "}\n",
    "\n",
    "# Model de classificació\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Configuració del grid search\n",
    "grid_search = GridSearchCV(\n",
    "    rf, \n",
    "    param_grid, \n",
    "    cv=10, \n",
    "    scoring='accuracy', \n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Dades del subdataset ICUT\n",
    "X_train_ICUT = filtered_datasets['ICUT']['X_train_filtrat']\n",
    "y_train_ICUT = filtered_datasets['ICUT']['y_train']\n",
    "\n",
    "# Entrenem el model\n",
    "grid_search.fit(X_train_ICUT, y_train_ICUT)\n",
    "\n",
    "# Resultats\n",
    "print(\"Millor paràmetre:\", grid_search.best_params_)\n",
    "print(\"Millor accuracy:\", grid_search.best_score_)\n",
    "\n",
    "print(\"Mitjana dels resultats de test per cada combinació:\")\n",
    "print(np.mean(grid_search.cv_results_['mean_test_score']))\n",
    "\n",
    "# Millor model final\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Importància de característiques\n",
    "coef_importants = best_model.feature_importances_\n",
    "feature_names = X_train_ICUT.columns  \n",
    "\n",
    "# Top 25 característiques més rellevants\n",
    "indices_ordenats = coef_importants.argsort()[::-1][:25]\n",
    "\n",
    "print(\"Les 25 característiques més importants són:\")\n",
    "for i in indices_ordenats:\n",
    "    print(f\"{feature_names[i]}: {coef_importants[i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculem el TRFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Millor paràmetre: {'max_depth': 30, 'n_estimators': 250}\n",
      "Millor accuracy: 0.5733333333333333\n",
      "Mitjana dels resultats de test per cada combinació:\n",
      "0.5544444444444444\n",
      "Les 25 característiques més importants són:\n",
      "TRFM_OP: 0.0243\n",
      "TRFM_SC: 0.0242\n",
      "TRFMAtteProb: 0.0232\n",
      "TRFMDSMCDT: 0.0224\n",
      "TRFMDSMStrProb: 0.0220\n",
      "TRFMTotProb: 0.0218\n",
      "TRFMExtProb: 0.0196\n",
      "TRFM_RBB: 0.0174\n",
      "TRFMTotProbT: 0.0172\n",
      "TRFMWithDepT: 0.0169\n",
      "TRFMRulBehT: 0.0167\n",
      "TRFMAggBeh: 0.0164\n",
      "TRFMRulBeh: 0.0160\n",
      "TRFM099: 0.0158\n",
      "TRFM_AP: 0.0157\n",
      "TRFMDSMStrProbT: 0.0155\n",
      "TRFMIntProbT: 0.0150\n",
      "TRFMWithDep: 0.0146\n",
      "TRFMAtteProbT: 0.0139\n",
      "TRFMAnxDepT: 0.0134\n",
      "TRFMIntProb: 0.0127\n",
      "TRFM_AB: 0.0126\n",
      "TRFM_TP: 0.0125\n",
      "TRFM022: 0.0115\n",
      "TRFMSocProb: 0.0115\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# Graella d'hiperparàmetres\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 250, 500],\n",
    "    'max_depth': [30, 50, 75],\n",
    "}\n",
    "\n",
    "# Model de classificació\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Configuració del grid search\n",
    "grid_search = GridSearchCV(\n",
    "    rf, \n",
    "    param_grid, \n",
    "    cv=10, \n",
    "    scoring='accuracy', \n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Dades del subdataset TRFM\n",
    "X_train_TRFM = filtered_datasets['TRFM']['X_train_filtrat']\n",
    "y_train_TRFM = filtered_datasets['TRFM']['y_train']\n",
    "\n",
    "# Entrenament\n",
    "grid_search.fit(X_train_TRFM, y_train_TRFM)\n",
    "\n",
    "# Resultats\n",
    "print(\"Millor paràmetre:\", grid_search.best_params_)\n",
    "print(\"Millor accuracy:\", grid_search.best_score_)\n",
    "\n",
    "print(\"Mitjana dels resultats de test per cada combinació:\")\n",
    "print(np.mean(grid_search.cv_results_['mean_test_score']))\n",
    "\n",
    "# Millor model final\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Importància de característiques\n",
    "coef_importants = best_model.feature_importances_\n",
    "feature_names = X_train_TRFM.columns  \n",
    "\n",
    "# Top 25 característiques més rellevants\n",
    "indices_ordenats = coef_importants.argsort()[::-1][:25]\n",
    "\n",
    "print(\"Les 25 característiques més importants són:\")\n",
    "for i in indices_ordenats:\n",
    "    print(f\"{feature_names[i]}: {coef_importants[i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ara fem el dataset de TRFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Millor paràmetre: {'max_depth': 30, 'n_estimators': 100}\n",
      "Millor accuracy: 0.6366666666666667\n",
      "Mitjana dels resultats de test per cada combinació:\n",
      "0.5544444444444445\n",
      "Les 25 característiques més importants són:\n",
      "TRFT_TP: 0.0241\n",
      "TRFTAggBehT: 0.0209\n",
      "TRFT_WD: 0.0209\n",
      "TRFMDSMADHDT: 0.0203\n",
      "TRFTTotProb: 0.0200\n",
      "TRFT_RBB: 0.0190\n",
      "TRFT041: 0.0181\n",
      "TRFMDSMDepProbT: 0.0170\n",
      "TRFTExtProb: 0.0169\n",
      "TRFMDSMADHD: 0.0169\n",
      "TRFTTotProbT: 0.0158\n",
      "TRFTRulBeh: 0.0155\n",
      "TRFTDSMStrProbT: 0.0154\n",
      "TRFTAtteProb: 0.0146\n",
      "TRFTDSMStrProb: 0.0145\n",
      "TRFT109: 0.0144\n",
      "TRFT_SP: 0.0142\n",
      "TRFMDSMDepProb: 0.0141\n",
      "TRFTExtProbT: 0.0139\n",
      "TRFTDSMODDT: 0.0137\n",
      "TRFTIntProb: 0.0132\n",
      "TRFT_OP: 0.0130\n",
      "TRFTAggBeh: 0.0130\n",
      "TRFTAnxDepT: 0.0127\n",
      "TRFTRulBehT: 0.0126\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# Graella d'hiperparàmetres\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 250, 500],\n",
    "    'max_depth': [30, 50, 75],\n",
    "}\n",
    "\n",
    "# Model de classificació\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Configuració del grid search\n",
    "grid_search = GridSearchCV(\n",
    "    rf, \n",
    "    param_grid, \n",
    "    cv=10, \n",
    "    scoring='accuracy', \n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Dades del subdataset TRFT\n",
    "X_train_TRFT = filtered_datasets['TRFT']['X_train_filtrat']\n",
    "y_train_TRFT = filtered_datasets['TRFT']['y_train']\n",
    "\n",
    "# Entrenament\n",
    "grid_search.fit(X_train_TRFT, y_train_TRFT)\n",
    "\n",
    "# Resultats\n",
    "print(\"Millor paràmetre:\", grid_search.best_params_)\n",
    "print(\"Millor accuracy:\", grid_search.best_score_)\n",
    "\n",
    "print(\"Mitjana dels resultats de test per cada combinació:\")\n",
    "print(np.mean(grid_search.cv_results_['mean_test_score']))\n",
    "\n",
    "# Millor model final\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Importància de característiques\n",
    "coef_importants = best_model.feature_importances_\n",
    "feature_names = X_train_TRFT.columns  \n",
    "\n",
    "# Top 25 característiques més rellevants\n",
    "indices_ordenats = coef_importants.argsort()[::-1][:25]\n",
    "\n",
    "print(\"Les 25 característiques més importants són:\")\n",
    "for i in indices_ordenats:\n",
    "    print(f\"{feature_names[i]}: {coef_importants[i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fem el dataset de YSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Millor paràmetre: {'max_depth': 30, 'n_estimators': 250}\n",
      "Millor accuracy: 0.47999999999999987\n",
      "Mitjana dels resultats de test per cada combinació:\n",
      "0.46111111111111097\n",
      "Les 25 característiques més importants són:\n",
      "TRFMAnxDep: 0.0632\n",
      "YSR054: 0.0212\n",
      "YSRSocProb: 0.0191\n",
      "YSRDSMAnxProb: 0.0159\n",
      "YSRSomComp: 0.0154\n",
      "YSRIntProbT: 0.0146\n",
      "YSR010: 0.0145\n",
      "YSRExtProbT: 0.0144\n",
      "YSRIntProb: 0.0140\n",
      "YSRAggBeh: 0.0132\n",
      "YSR026: 0.0130\n",
      "YSRSomCompT: 0.0128\n",
      "YSR005: 0.0125\n",
      "YSRTotProb: 0.0122\n",
      "YSRSocProbT: 0.0117\n",
      "YSRDSMDepProb: 0.0117\n",
      "YSRDSMCDT: 0.0114\n",
      "YSR003: 0.0113\n",
      "YSRDSMStrProb: 0.0113\n",
      "YSRDSMADHD: 0.0107\n",
      "YSRExtProb: 0.0107\n",
      "YSRAtteProb: 0.0104\n",
      "YSR111: 0.0103\n",
      "YSRThouProbT: 0.0099\n",
      "YSRWithDepT: 0.0093\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# Graella d'hiperparàmetres\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 250, 500],\n",
    "    'max_depth': [30, 50, 75],\n",
    "}\n",
    "\n",
    "# Model de classificació\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Configuració del grid search\n",
    "grid_search = GridSearchCV(\n",
    "    rf, \n",
    "    param_grid, \n",
    "    cv=10, \n",
    "    scoring='accuracy', \n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Dades del subdataset YSR\n",
    "X_train_YSR = filtered_datasets['YSR']['X_train_filtrat']\n",
    "y_train_YSR = filtered_datasets['YSR']['y_train']\n",
    "\n",
    "# Entrenament\n",
    "grid_search.fit(X_train_YSR, y_train_YSR)\n",
    "\n",
    "# Resultats\n",
    "print(\"Millor paràmetre:\", grid_search.best_params_)\n",
    "print(\"Millor accuracy:\", grid_search.best_score_)\n",
    "\n",
    "print(\"Mitjana dels resultats de test per cada combinació:\")\n",
    "print(np.mean(grid_search.cv_results_['mean_test_score']))\n",
    "\n",
    "# Millor model final\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Importància de característiques\n",
    "coef_importants = best_model.feature_importances_\n",
    "feature_names = X_train_YSR.columns  \n",
    "\n",
    "# Top 25 característiques més rellevants\n",
    "indices_ordenats = coef_importants.argsort()[::-1][:25]\n",
    "\n",
    "print(\"Les 25 característiques més importants són:\")\n",
    "for i in indices_ordenats:\n",
    "    print(f\"{feature_names[i]}: {coef_importants[i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ara farem la triada fosca: Tri_PM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Millor paràmetre: {'max_depth': 75, 'n_estimators': 500}\n",
      "Millor accuracy: 0.5666666666666665\n",
      "Mitjana dels resultats de test per cada combinació:\n",
      "0.561111111111111\n",
      "Les 25 característiques més importants són:\n",
      "TriPMMean: 0.0320\n",
      "TriPMBold: 0.0308\n",
      "TriPMTotal: 0.0270\n",
      "TriPM44: 0.0247\n",
      "TriPMDisi: 0.0246\n",
      "xTriPM44: 0.0223\n",
      "TriPM49: 0.0216\n",
      "xTriPM04: 0.0193\n",
      "TriPM15: 0.0186\n",
      "TriPM51: 0.0169\n",
      "TriPM08: 0.0166\n",
      "TriPM37: 0.0163\n",
      "xTriPM35: 0.0162\n",
      "xTriPM11: 0.0162\n",
      "TriPM06: 0.0161\n",
      "TriPM29: 0.0158\n",
      "xTriPM41: 0.0151\n",
      "TriPM04: 0.0150\n",
      "TriPM22: 0.0149\n",
      "xTriPM33: 0.0144\n",
      "TriPM31: 0.0144\n",
      "TriPM33: 0.0144\n",
      "TriPM13: 0.0142\n",
      "TriPM54: 0.0142\n",
      "TriPM27: 0.0137\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# Graella d'hiperparàmetres\n",
    "param_grid = {\n",
    "    'n_estimators': [500, 600, 750],\n",
    "    'max_depth': [75, 90, 100],\n",
    "}\n",
    "\n",
    "# Model de classificació\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Configuració del grid search\n",
    "grid_search = GridSearchCV(\n",
    "    rf,\n",
    "    param_grid,\n",
    "    cv=10,\n",
    "    scoring='accuracy',\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Dades del subdataset TriPM\n",
    "X_train_TriPM = filtered_datasets['TriPM']['X_train_filtrat']\n",
    "y_train_TriPM = filtered_datasets['TriPM']['y_train']\n",
    "\n",
    "# Entrenament\n",
    "grid_search.fit(X_train_TriPM, y_train_TriPM)\n",
    "\n",
    "# Resultats\n",
    "print(\"Millor paràmetre:\", grid_search.best_params_)\n",
    "print(\"Millor accuracy:\", grid_search.best_score_)\n",
    "\n",
    "print(\"Mitjana dels resultats de test per cada combinació:\")\n",
    "print(np.mean(grid_search.cv_results_['mean_test_score']))\n",
    "\n",
    "# Millor model final\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Importància de característiques\n",
    "coef_importants = best_model.feature_importances_\n",
    "feature_names = X_train_TriPM.columns\n",
    "\n",
    "# Top 25 característiques més rellevants\n",
    "indices_ordenats = coef_importants.argsort()[::-1][:25]\n",
    "\n",
    "print(\"Les 25 característiques més importants són:\")\n",
    "for i in indices_ordenats:\n",
    "    print(f\"{feature_names[i]}: {coef_importants[i]:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
