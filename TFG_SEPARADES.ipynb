{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crear_dataset import *\n",
    "from funcions_net import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyreadstat\n",
    "import pandas as pd\n",
    "from preprocessing import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from funcions_net import *\n",
    "from datasets_separats import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['temps_fins_reincidencia1a', 'YPI01', 'YPI02', 'YPI03', 'YPI04',\n",
      "       'YPI05', 'YPI06', 'YPI07', 'YPI08', 'YPI09', 'YPI10', 'YPI11', 'YPI12',\n",
      "       'YPI13', 'YPI14', 'YPI15', 'YPI16', 'YPI17', 'YPI18', 'YPI19', 'YPI20',\n",
      "       'YPI21', 'YPI22', 'YPI23', 'YPI24', 'YPI25', 'YPI26', 'YPI27', 'YPI28',\n",
      "       'YPI29', 'YPI30', 'YPI31', 'YPI32', 'YPI33', 'YPI34', 'YPI35', 'YPI36',\n",
      "       'YPI37', 'YPI38', 'YPI39', 'YPI40', 'YPI41', 'YPI42', 'YPI43', 'YPI44',\n",
      "       'YPI45', 'YPI46', 'YPI47', 'YPI48', 'YPI49', 'YPI50', 'xYPI23',\n",
      "       'xYPI35', 'xYPI49', 'YPI_DC', 'YPI_GR', 'YPI_LY', 'YPI_MA', 'YPI_RE',\n",
      "       'YPI_UN', 'YPI_CA', 'YPI_TS', 'YPI_IM', 'YPI_IR', 'YPI_PT', 'YPI_GM',\n",
      "       'YPI_CU', 'YPI_II', 'YPI_PT_3gr', 'YPI_GM_3gr', 'YPI_CU_3gr',\n",
      "       'YPI_II_3gr', 'YPI_PT_2gr', 'YPI_GM_2gr', 'YPI_CU_2gr', 'YPI_II_2gr'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_orig, meta = pyreadstat.read_sav(\"CEJFEAjut2015Updated.sav\")\n",
    "\n",
    "df_variables = pd.read_csv(\"variables.csv\", sep=';')\n",
    "\n",
    "dict = create_dict(meta)\n",
    "\n",
    "datasets = dataset_dif2(df_orig, dict,df_variables)\n",
    "\n",
    "print(datasets[\"YPI\"].columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\pipilenes.py:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_copy[col].fillna(X_copy[col].mean(), inplace=True)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\pipilenes.py:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_copy[col].fillna(X_copy[col].mean(), inplace=True)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\pipilenes.py:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_copy[col].fillna(X_copy[col].mean(), inplace=True)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\pipilenes.py:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_copy[col].fillna(X_copy[col].mean(), inplace=True)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\pipilenes.py:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_copy[col].fillna(X_copy[col].mean(), inplace=True)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\pipilenes.py:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_copy[col].fillna(X_copy[col].mean(), inplace=True)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\pipilenes.py:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_copy[col].fillna(X_copy[col].mean(), inplace=True)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\pipilenes.py:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_copy[col].fillna(X_copy[col].mean(), inplace=True)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\pipilenes.py:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_copy[col].fillna(X_copy[col].mean(), inplace=True)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\pipilenes.py:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_copy[col].fillna(X_copy[col].mean(), inplace=True)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\pipilenes.py:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_copy[col].fillna(X_copy[col].mean(), inplace=True)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\pipilenes.py:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_copy[col].fillna(X_copy[col].mean(), inplace=True)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\pipilenes.py:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_copy[col].fillna(X_copy[col].mean(), inplace=True)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\pipilenes.py:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_copy[col].fillna(X_copy[col].mean(), inplace=True)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\pipilenes.py:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_copy[col].fillna(X_copy[col].mean(), inplace=True)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\pipilenes.py:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_copy[col].fillna(X_copy[col].mean(), inplace=True)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\pipilenes.py:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_copy[col].fillna(X_copy[col].mean(), inplace=True)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_test_splits = split_datasets(datasets)  # Apliquem la separaci\n",
    "transformed_train_sets = apply_pipeline_to_train_sets(train_test_splits)  # Apliquem la pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RAVEN01</th>\n",
       "      <th>RAVEN02</th>\n",
       "      <th>RAVEN03</th>\n",
       "      <th>RAVEN04</th>\n",
       "      <th>RAVEN05</th>\n",
       "      <th>RAVEN_PD</th>\n",
       "      <th>RAVEN_Pc</th>\n",
       "      <th>RAVEN_IQ</th>\n",
       "      <th>RAVENbarem</th>\n",
       "      <th>RavenZ</th>\n",
       "      <th>Raven_IQ2</th>\n",
       "      <th>RavenZ_2</th>\n",
       "      <th>Raven_IQ3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-2.067064</td>\n",
       "      <td>68.994039</td>\n",
       "      <td>-2.178248</td>\n",
       "      <td>67.326284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.615499</td>\n",
       "      <td>109.232489</td>\n",
       "      <td>0.340872</td>\n",
       "      <td>105.113086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.168405</td>\n",
       "      <td>102.526080</td>\n",
       "      <td>0.443325</td>\n",
       "      <td>106.649874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-5.047690</td>\n",
       "      <td>24.284650</td>\n",
       "      <td>-3.964736</td>\n",
       "      <td>40.528967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>11.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.274637</td>\n",
       "      <td>80.880452</td>\n",
       "      <td>-1.274637</td>\n",
       "      <td>80.880452</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    RAVEN01  RAVEN02  RAVEN03  RAVEN04  RAVEN05  RAVEN_PD  RAVEN_Pc  RAVEN_IQ  \\\n",
       "31      7.0      7.0      5.0     10.0      3.0      32.0       5.0      76.0   \n",
       "23     10.0     10.0     11.0     10.0      9.0      50.0      75.0     110.0   \n",
       "58     12.0     12.0     11.0     11.0      1.0      47.0      50.0     100.0   \n",
       "17      6.0      1.0      2.0      2.0      1.0      12.0       3.0      70.0   \n",
       "69     11.0     10.0      8.0      9.0      2.0      40.0      50.0      90.0   \n",
       "\n",
       "    RAVENbarem    RavenZ   Raven_IQ2  RavenZ_2   Raven_IQ3  \n",
       "31         2.0 -2.067064   68.994039 -2.178248   67.326284  \n",
       "23         2.0  0.615499  109.232489  0.340872  105.113086  \n",
       "58         2.0  0.168405  102.526080  0.443325  106.649874  \n",
       "17         2.0 -5.047690   24.284650 -3.964736   40.528967  \n",
       "69         1.0 -1.274637   80.880452 -1.274637   80.880452  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_train_sets[\"IQ\"]['X_train'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'datasets_separats' from 'c:\\\\Users\\\\jjf20\\\\OneDrive\\\\Documents\\\\Universitat\\\\TFG\\\\datasets_separats.py'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets_separats\n",
    "import importlib\n",
    "importlib.reload(datasets_separats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrem a la funci:\n",
      "Applying significant features filter to dataset IQ...\n",
      "Tamanys train (57, 13)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 13)\n",
      "Tamanys train (57, 13)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 13)\n",
      "Applying significant features filter to dataset SAVRY...\n",
      "Tamanys train (57, 42)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 42)\n",
      "Tamanys train (57, 42)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 42)\n",
      "Applying significant features filter to dataset VAST...\n",
      "Tamanys train (57, 23)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 23)\n",
      "Tamanys train (57, 23)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 23)\n",
      "Applying significant features filter to dataset PCL...\n",
      "Tamanys train (57, 141)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 141)\n",
      "Tamanys train (57, 141)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 141)\n",
      "Applying significant features filter to dataset CAPE...\n",
      "Tamanys train (57, 17)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 17)\n",
      "Tamanys train (57, 17)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 17)\n",
      "Applying significant features filter to dataset YPI...\n",
      "Tamanys train (57, 75)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 75)\n",
      "Tamanys train (57, 75)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 75)\n",
      "Applying significant features filter to dataset RPQ...\n",
      "Tamanys train (57, 29)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 29)\n",
      "Tamanys train (57, 29)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 29)\n",
      "Applying significant features filter to dataset CCA...\n",
      "Tamanys train (57, 40)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 40)\n",
      "Tamanys train (57, 40)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 40)\n",
      "Applying significant features filter to dataset SD3...\n",
      "Tamanys train (57, 39)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 39)\n",
      "Tamanys train (57, 39)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 39)\n",
      "Applying significant features filter to dataset ICUJ...\n",
      "Tamanys train (57, 78)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 78)\n",
      "Tamanys train (57, 78)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 78)\n",
      "Applying significant features filter to dataset ICUT...\n",
      "Tamanys train (57, 30)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 30)\n",
      "Tamanys train (57, 30)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 30)\n",
      "Applying significant features filter to dataset TRFM...\n",
      "Tamanys train (57, 173)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 173)\n",
      "Tamanys train (57, 173)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 173)\n",
      "Applying significant features filter to dataset YSR...\n",
      "Tamanys train (57, 181)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 181)\n",
      "Tamanys train (57, 181)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 181)\n",
      "Applying significant features filter to dataset TriPM...\n",
      "Tamanys train (57, 87)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 87)\n",
      "Tamanys train (57, 87)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 87)\n",
      "Applying significant features filter to dataset TRFT...\n",
      "Tamanys train (57, 197)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 197)\n",
      "Tamanys train (57, 197)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 197)\n",
      "Applying significant features filter to dataset DD...\n",
      "Tamanys train (57, 24)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 24)\n",
      "Tamanys train (57, 24)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 24)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.923e+06, tolerance: 4.510e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.296e+05, tolerance: 4.510e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.243e+05, tolerance: 4.510e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.660e+04, tolerance: 4.510e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.063e+05, tolerance: 4.510e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.653e+04, tolerance: 4.510e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.152e+04, tolerance: 4.510e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.853e+05, tolerance: 4.510e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.500e+04, tolerance: 4.510e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.577e+04, tolerance: 4.510e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.257e+04, tolerance: 4.510e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.695e+04, tolerance: 4.510e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "filtered_datasets = datasets_separats.apply_significant_features_filter(transformed_train_sets, df_orig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Millor parmetre: {'max_depth': 50, 'n_estimators': 250}\n",
      "Millor precisi: 101505.30808600248\n",
      "Mitjana dels resultats de test per cada combinaci:\n",
      "103494.1985661582\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "## fem grid search per trobar els millors hiperparmetres\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100,250,500],\n",
    "    'max_depth': [30,50,75],\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    rf, \n",
    "    param_grid, \n",
    "    cv=10, \n",
    "    scoring='neg_mean_squared_error', \n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "X_train_IQ = filtered_datasets['IQ']['X_train_filtrat']\n",
    "y_train_IQ = filtered_datasets['IQ']['y_train']\n",
    "\n",
    "# Realitzem el Grid Search\n",
    "grid_search.fit(X_train_IQ, y_train_IQ)\n",
    "\n",
    "# Resultats\n",
    "print(\"Millor parmetre:\", grid_search.best_params_)\n",
    "print(\"Millor precisi:\", -grid_search.best_score_)  # Negatiu perqu s'ha usat neg_mean_squared_error\n",
    "\n",
    "# Mostrem la mitjana dels resultats de test\n",
    "print(\"Mitjana dels resultats de test per cada combinaci:\")\n",
    "print(np.mean(grid_search.cv_results_['mean_test_score'] * -1))  # Multipliquem per -1 per obtenir el MSE positiu\n",
    "\n",
    "best_model = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les 20 caracterstiques ms importants sn:\n",
      "RAVEN05: 0.1733\n",
      "RavenZ_2: 0.1391\n",
      "RAVEN04: 0.1279\n",
      "Raven_IQ3: 0.1101\n",
      "RAVEN03: 0.0836\n",
      "RAVEN_PD: 0.0783\n",
      "RAVEN02: 0.0684\n",
      "Raven_IQ2: 0.0657\n",
      "RavenZ: 0.0544\n",
      "RAVEN01: 0.0411\n",
      "RAVEN_IQ: 0.0247\n",
      "RAVEN_Pc: 0.0244\n",
      "RAVENbarem: 0.0090\n"
     ]
    }
   ],
   "source": [
    "# Obtenim la importncia de cada caracterstica\n",
    "coef_importants = best_model.feature_importances_\n",
    "\n",
    "# Obtenim els noms de les caracterstiques (suposant que tenim un DataFrame 'df')\n",
    "feature_names = X_train_IQ.columns  \n",
    "\n",
    "# Ordenem les caracterstiques segons la seva importncia (de major a menor)\n",
    "indices_ordenats = coef_importants.argsort()[::-1][0:25]\n",
    "\n",
    "# Mostrem els noms de les 20 caracterstiques ms importants amb la seva importncia\n",
    "print(\"Les 20 caracterstiques ms importants sn:\")\n",
    "for i in indices_ordenats:\n",
    "    print(f\"{feature_names[i]}: {coef_importants[i]:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les millors caracterstiques son RAVEN05 i RAVEN02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ara comencem amb el segon dataset que s SAVRY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Millor parmetre: {'max_depth': 20, 'n_estimators': 900}\n",
      "Millor precisi: 76330.16659849383\n",
      "Mitjana dels resultats de test per cada combinaci:\n",
      "77586.38044545265\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [500,700,900],\n",
    "    'max_depth': [10,20,30],\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    rf, \n",
    "    param_grid, \n",
    "    cv=10, \n",
    "    scoring='neg_mean_squared_error', \n",
    "    return_train_score=True\n",
    ")\n",
    "X_train_SAVRY = filtered_datasets['SAVRY']['X_train_filtrat']\n",
    "y_train_SAVRY = filtered_datasets['SAVRY']['y_train']\n",
    "\n",
    "# Realitzem el Grid Search\n",
    "grid_search.fit(X_train_SAVRY, y_train_SAVRY)\n",
    "\n",
    "# Resultats\n",
    "print(\"Millor parmetre:\", grid_search.best_params_)\n",
    "print(\"Millor precisi:\", -grid_search.best_score_)  # Negatiu perqu s'ha usat neg_mean_squared_error\n",
    "\n",
    "# Mostrem la mitjana dels resultats de test\n",
    "print(\"Mitjana dels resultats de test per cada combinaci:\")\n",
    "print(np.mean(grid_search.cv_results_['mean_test_score'] * -1))  # Multipliquem per -1 per obtenir el MSE positiu\n",
    "\n",
    "best_model = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les 20 caracterstiques ms importants sn:\n",
      "SAVRY02: 0.1870\n",
      "SAVRYSoc: 0.0786\n",
      "SAVRY11: 0.0695\n",
      "SAVRY05: 0.0685\n",
      "SAVRYTot: 0.0626\n",
      "SAVRY25: 0.0558\n",
      "SAVRYHis: 0.0454\n",
      "SAVRY04: 0.0419\n",
      "SAVRYInd: 0.0366\n",
      "SAVRY36: 0.0296\n",
      "SAVRY38: 0.0233\n",
      "SAVRY29: 0.0202\n",
      "SAVRY08: 0.0175\n",
      "SAVRY01: 0.0172\n",
      "SAVRY28: 0.0157\n",
      "SAVRY20: 0.0154\n",
      "SAVRY30: 0.0153\n",
      "SAVRY27: 0.0150\n",
      "SAVRY09: 0.0148\n",
      "SAVRY18: 0.0146\n",
      "SAVRY13: 0.0137\n",
      "SAVRY16: 0.0121\n",
      "SAVRY15: 0.0113\n",
      "SAVRY39: 0.0107\n",
      "SAVRY21: 0.0105\n"
     ]
    }
   ],
   "source": [
    "# Obtenim la importncia de cada caracterstica\n",
    "coef_importants = best_model.feature_importances_\n",
    "\n",
    "# Obtenim els noms de les caracterstiques (suposant que tenim un DataFrame 'df')\n",
    "feature_names = X_train_SAVRY.columns  \n",
    "\n",
    "# Ordenem les caracterstiques segons la seva importncia (de major a menor)\n",
    "indices_ordenats = coef_importants.argsort()[::-1][0:25]\n",
    "\n",
    "# Mostrem els noms de les 20 caracterstiques ms importants amb la seva importncia\n",
    "print(\"Les 20 caracterstiques ms importants sn:\")\n",
    "for i in indices_ordenats:\n",
    "    print(f\"{feature_names[i]}: {coef_importants[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les millos caractqristiques\n",
    "SAVRYHis: 0.1155\n",
    "SAVRY05: 0.1019\n",
    "SAVRYTot: 0.0970"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fem ara el model DD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Millor parmetre: {'max_depth': 30, 'n_estimators': 500}\n",
      "Millor precisi: 110740.10380762618\n",
      "Mitjana dels resultats de test per cada combinaci:\n",
      "112934.70358308073\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [500,700,900],\n",
    "    'max_depth': [10,20,30],\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    rf, \n",
    "    param_grid, \n",
    "    cv=10, \n",
    "    scoring='neg_mean_squared_error', \n",
    "    return_train_score=True\n",
    ")\n",
    "X_train_DD = filtered_datasets['DD']['X_train_filtrat']\n",
    "y_train_DD = filtered_datasets['DD']['y_train']\n",
    "\n",
    "# Realitzem el Grid Search\n",
    "grid_search.fit(X_train_DD, y_train_DD)\n",
    "\n",
    "# Resultats\n",
    "print(\"Millor parmetre:\", grid_search.best_params_)\n",
    "print(\"Millor precisi:\", -grid_search.best_score_)  # Negatiu perqu s'ha usat neg_mean_squared_error\n",
    "\n",
    "# Mostrem la mitjana dels resultats de test\n",
    "print(\"Mitjana dels resultats de test per cada combinaci:\")\n",
    "print(np.mean(grid_search.cv_results_['mean_test_score'] * -1))  # Multipliquem per -1 per obtenir el MSE positiu\n",
    "\n",
    "best_model = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les 20 caracterstiques ms importants sn:\n",
      "DD02: 0.1350\n",
      "DD08: 0.1161\n",
      "DD05: 0.0880\n",
      "DDNarc: 0.0858\n",
      "DDPsyc: 0.0638\n",
      "DD03: 0.0619\n",
      "DD07: 0.0607\n",
      "DD04: 0.0575\n",
      "DD01: 0.0537\n",
      "DDMach: 0.0493\n",
      "DD09: 0.0449\n",
      "DD06: 0.0363\n",
      "DDTotal_3gr: 0.0344\n",
      "DD12: 0.0256\n",
      "DDPsyc_3gr: 0.0198\n",
      "DDMach_3gr: 0.0192\n",
      "DDNarc_3gr: 0.0184\n",
      "DDPsyc_2gr: 0.0084\n",
      "DDMach_2gr: 0.0082\n",
      "DDTotal_2gr: 0.0078\n",
      "DDNarc_2gr: 0.0054\n"
     ]
    }
   ],
   "source": [
    "# Obtenim la importncia de cada caracterstica\n",
    "coef_importants = best_model.feature_importances_\n",
    "\n",
    "# Obtenim els noms de les caracterstiques (suposant que tenim un DataFrame 'df')\n",
    "feature_names = X_train_DD.columns  \n",
    "\n",
    "# Ordenem les caracterstiques segons la seva importncia (de major a menor)\n",
    "indices_ordenats = coef_importants.argsort()[::-1][0:25]\n",
    "\n",
    "# Mostrem els noms de les 20 caracterstiques ms importants amb la seva importncia\n",
    "print(\"Les 20 caracterstiques ms importants sn:\")\n",
    "for i in indices_ordenats:\n",
    "    print(f\"{feature_names[i]}: {coef_importants[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ara farem VAST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Millor parmetre: {'max_depth': 15, 'n_estimators': 100}\n",
      "Millor precisi: 91057.3631693837\n",
      "Mitjana dels resultats de test per cada combinaci:\n",
      "96837.61777334922\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [50,100,200],\n",
    "    'max_depth': [15,30,45],\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    rf, \n",
    "    param_grid, \n",
    "    cv=10, \n",
    "    scoring='neg_mean_squared_error', \n",
    "    return_train_score=True\n",
    ")\n",
    "X_train_VAST = filtered_datasets['VAST']['X_train_filtrat']\n",
    "y_train_VAST = filtered_datasets['VAST']['y_train']\n",
    "\n",
    "# Realitzem el Grid Search\n",
    "grid_search.fit(X_train_VAST, y_train_VAST)\n",
    "\n",
    "# Resultats\n",
    "print(\"Millor parmetre:\", grid_search.best_params_)\n",
    "print(\"Millor precisi:\", -grid_search.best_score_)  # Negatiu perqu s'ha usat neg_mean_squared_error\n",
    "\n",
    "# Mostrem la mitjana dels resultats de test\n",
    "print(\"Mitjana dels resultats de test per cada combinaci:\")\n",
    "print(np.mean(grid_search.cv_results_['mean_test_score'] * -1))  # Multipliquem per -1 per obtenir el MSE positiu\n",
    "\n",
    "best_model = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les 20 caracterstiques ms importants sn:\n",
      "VAST18: 0.0969\n",
      "VAST14: 0.0943\n",
      "VASTDir: 0.0894\n",
      "VAST10: 0.0794\n",
      "VAST17: 0.0751\n",
      "VAST12: 0.0724\n",
      "VAST01: 0.0647\n",
      "VAST06: 0.0543\n",
      "VAST02: 0.0467\n",
      "VASTVic: 0.0423\n",
      "VASTTotal: 0.0414\n",
      "VAST19: 0.0370\n",
      "VAST08: 0.0362\n",
      "VAST03: 0.0304\n",
      "VAST11: 0.0233\n",
      "VAST13: 0.0195\n",
      "VAST05: 0.0181\n",
      "VAST04: 0.0177\n",
      "VAST09: 0.0154\n",
      "VAST15: 0.0148\n",
      "VAST20: 0.0120\n",
      "VAST07: 0.0102\n",
      "VAST16: 0.0084\n"
     ]
    }
   ],
   "source": [
    "# Obtenim la importncia de cada caracterstica\n",
    "coef_importants = best_model.feature_importances_\n",
    "\n",
    "# Obtenim els noms de les caracterstiques (suposant que tenim un DataFrame 'df')\n",
    "feature_names = X_train_VAST.columns  \n",
    "\n",
    "# Ordenem les caracterstiques segons la seva importncia (de major a menor)\n",
    "indices_ordenats = coef_importants.argsort()[::-1][0:25]\n",
    "\n",
    "# Mostrem els noms de les 20 caracterstiques ms importants amb la seva importncia\n",
    "print(\"Les 20 caracterstiques ms importants sn:\")\n",
    "for i in indices_ordenats:\n",
    "    print(f\"{feature_names[i]}: {coef_importants[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fem el dataset de PCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Millor parmetre: {'max_depth': 30, 'n_estimators': 25}\n",
      "Millor precisi: 64323.10539812003\n",
      "Mitjana dels resultats de test per cada combinaci:\n",
      "74157.65885676327\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [25,50,100],\n",
    "    'max_depth': [30,50,75],\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    rf, \n",
    "    param_grid, \n",
    "    cv=10, \n",
    "    scoring='neg_mean_squared_error', \n",
    "    return_train_score=True\n",
    ")\n",
    "X_train_PCL = filtered_datasets['PCL']['X_train_filtrat']\n",
    "y_train_PCL = filtered_datasets['PCL']['y_train']\n",
    "\n",
    "# Realitzem el Grid Search\n",
    "grid_search.fit(X_train_PCL, y_train_PCL)\n",
    "\n",
    "# Resultats\n",
    "print(\"Millor parmetre:\", grid_search.best_params_)\n",
    "print(\"Millor precisi:\", -grid_search.best_score_)  # Negatiu perqu s'ha usat neg_mean_squared_error\n",
    "\n",
    "# Mostrem la mitjana dels resultats de test\n",
    "print(\"Mitjana dels resultats de test per cada combinaci:\")\n",
    "print(np.mean(grid_search.cv_results_['mean_test_score'] * -1))  # Multipliquem per -1 per obtenir el MSE positiu\n",
    "\n",
    "best_model = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les 20 caracterstiques ms importants sn:\n",
      "PCLjFAC_3: 0.3509\n",
      "PCLj14: 0.0528\n",
      "PCLjFAC_1: 0.0495\n",
      "PCLjFAC_2: 0.0483\n",
      "PCLeFAC_2: 0.0381\n",
      "PCLj12: 0.0343\n",
      "PCLoFAC_1: 0.0285\n",
      "PCLo13: 0.0276\n",
      "PCLj11: 0.0252\n",
      "PCLo10: 0.0183\n",
      "PCLo_PT: 0.0181\n",
      "PCLo05: 0.0147\n",
      "PCLx09: 0.0139\n",
      "PCLe16: 0.0138\n",
      "PCLe_PT: 0.0123\n",
      "PCLe08: 0.0122\n",
      "PCLeFAC_3: 0.0119\n",
      "PCLYVMPT: 0.0112\n",
      "PCLx16: 0.0112\n",
      "PCLo09: 0.0110\n",
      "PCLxFAC_1: 0.0106\n",
      "PCLYVMFAC_4_3gr: 0.0103\n",
      "PCLxFAC_3: 0.0097\n",
      "PCLoFAC_2: 0.0094\n",
      "PCLoFAC_4: 0.0077\n"
     ]
    }
   ],
   "source": [
    "# Obtenim la importncia de cada caracterstica\n",
    "coef_importants = best_model.feature_importances_\n",
    "\n",
    "# Obtenim els noms de les caracterstiques (suposant que tenim un DataFrame 'df')\n",
    "feature_names = X_train_PCL.columns  \n",
    "\n",
    "# Ordenem les caracterstiques segons la seva importncia (de major a menor)\n",
    "indices_ordenats = coef_importants.argsort()[::-1][0:25]\n",
    "\n",
    "# Mostrem els noms de les 20 caracterstiques ms importants amb la seva importncia\n",
    "print(\"Les 20 caracterstiques ms importants sn:\")\n",
    "for i in indices_ordenats:\n",
    "    print(f\"{feature_names[i]}: {coef_importants[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claus amb valor PCLjFAC_3: [258]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "valor_que_busco = \"PCLjFAC_3\"\n",
    "\n",
    "claus = [clau for clau, valor in dict.items() if valor == valor_que_busco]\n",
    "\n",
    "print(f\"Claus amb valor {valor_que_busco}: {claus}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ara anem a fer CAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Millor parmetre: {'max_depth': 75, 'n_estimators': 100}\n",
      "Millor precisi: 77951.54423988292\n",
      "Mitjana dels resultats de test per cada combinaci:\n",
      "80758.97468973516\n",
      "Les 20 caracterstiques ms importants sn:\n",
      "CAPEe03: 0.4243\n",
      "CAPEe04: 0.0897\n",
      "CAPEe01: 0.0803\n",
      "CAPEo201: 0.0606\n",
      "CAPEo01: 0.0568\n",
      "CAPEo202: 0.0509\n",
      "CAPEo203: 0.0386\n",
      "CAPEo02: 0.0374\n",
      "CAPEo04: 0.0356\n",
      "CAPEo03: 0.0283\n",
      "CAPEe02: 0.0264\n",
      "CAPEe_PT_2gr: 0.0163\n",
      "CAPEo_PT_3gr: 0.0159\n",
      "CAPEo204: 0.0156\n",
      "CAPEo_PT_2gr: 0.0135\n",
      "CAPEe_PT_3gr: 0.0098\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100,250,500],\n",
    "    'max_depth': [30,50,75],\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    rf, \n",
    "    param_grid, \n",
    "    cv=10, \n",
    "    scoring='neg_mean_squared_error', \n",
    "    return_train_score=True\n",
    ")\n",
    "X_train_CAPE = filtered_datasets['CAPE']['X_train_filtrat']\n",
    "y_train_CAPE = filtered_datasets['CAPE']['y_train']\n",
    "\n",
    "# Realitzem el Grid Search\n",
    "grid_search.fit(X_train_CAPE, y_train_CAPE)\n",
    "\n",
    "# Resultats\n",
    "print(\"Millor parmetre:\", grid_search.best_params_)\n",
    "print(\"Millor precisi:\", -grid_search.best_score_)  # Negatiu perqu s'ha usat neg_mean_squared_error\n",
    "\n",
    "# Mostrem la mitjana dels resultats de test\n",
    "print(\"Mitjana dels resultats de test per cada combinaci:\")\n",
    "print(np.mean(grid_search.cv_results_['mean_test_score'] * -1))  # Multipliquem per -1 per obtenir el MSE positiu\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Obtenim la importncia de cada caracterstica\n",
    "coef_importants = best_model.feature_importances_\n",
    "\n",
    "# Obtenim els noms de les caracterstiques (suposant que tenim un DataFrame 'df')\n",
    "feature_names = X_train_CAPE.columns  \n",
    "\n",
    "# Ordenem les caracterstiques segons la seva importncia (de major a menor)\n",
    "indices_ordenats = coef_importants.argsort()[::-1][0:25]\n",
    "\n",
    "# Mostrem els noms de les 20 caracterstiques ms importants amb la seva importncia\n",
    "print(\"Les 20 caracterstiques ms importants sn:\")\n",
    "for i in indices_ordenats:\n",
    "    print(f\"{feature_names[i]}: {coef_importants[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ara anem a fer YPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Millor parmetre: {'max_depth': 20, 'n_estimators': 100}\n",
      "Millor precisi: 78064.09418793334\n",
      "Mitjana dels resultats de test per cada combinaci:\n",
      "79980.0777389698\n",
      "Les 20 caracterstiques ms importants sn:\n",
      "YPI32: 0.0814\n",
      "YPI24: 0.0772\n",
      "YPI05: 0.0762\n",
      "YPI09: 0.0554\n",
      "YPI33: 0.0482\n",
      "YPI04: 0.0386\n",
      "YPI13: 0.0310\n",
      "YPI10: 0.0280\n",
      "YPI15: 0.0270\n",
      "YPI39: 0.0253\n",
      "xYPI35: 0.0203\n",
      "YPI25: 0.0191\n",
      "YPI46: 0.0190\n",
      "YPI11: 0.0190\n",
      "YPI17: 0.0190\n",
      "YPI47: 0.0184\n",
      "YPI07: 0.0183\n",
      "YPI_CU_3gr: 0.0178\n",
      "YPI29: 0.0168\n",
      "YPI03: 0.0167\n",
      "YPI36: 0.0161\n",
      "YPI_GM_3gr: 0.0156\n",
      "YPI42: 0.0155\n",
      "YPI49: 0.0135\n",
      "YPI50: 0.0134\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100,250,500],\n",
    "    'max_depth': [20,30,50],\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    rf, \n",
    "    param_grid, \n",
    "    cv=10, \n",
    "    scoring='neg_mean_squared_error', \n",
    "    return_train_score=True\n",
    ")\n",
    "X_train_YPI = filtered_datasets['YPI']['X_train_filtrat']\n",
    "y_train_YPI = filtered_datasets['YPI']['y_train']\n",
    "\n",
    "# Realitzem el Grid Search\n",
    "grid_search.fit(X_train_YPI, y_train_YPI)\n",
    "\n",
    "# Resultats\n",
    "print(\"Millor parmetre:\", grid_search.best_params_)\n",
    "print(\"Millor precisi:\", -grid_search.best_score_)  # Negatiu perqu s'ha usat neg_mean_squared_error\n",
    "\n",
    "# Mostrem la mitjana dels resultats de test\n",
    "print(\"Mitjana dels resultats de test per cada combinaci:\")\n",
    "print(np.mean(grid_search.cv_results_['mean_test_score'] * -1))  # Multipliquem per -1 per obtenir el MSE positiu\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Obtenim la importncia de cada caracterstica\n",
    "coef_importants = best_model.feature_importances_\n",
    "\n",
    "# Obtenim els noms de les caracterstiques (suposant que tenim un DataFrame 'df')\n",
    "feature_names = X_train_YPI.columns  \n",
    "\n",
    "# Ordenem les caracterstiques segons la seva importncia (de major a menor)\n",
    "indices_ordenats = coef_importants.argsort()[::-1][0:25]\n",
    "\n",
    "# Mostrem els noms de les 20 caracterstiques ms importants amb la seva importncia\n",
    "print(\"Les 20 caracterstiques ms importants sn:\")\n",
    "for i in indices_ordenats:\n",
    "    print(f\"{feature_names[i]}: {coef_importants[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Farem servir servir el dataset RPQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Millor parmetre: {'max_depth': 75, 'n_estimators': 500}\n",
      "Millor precisi: 100724.08727673178\n",
      "Mitjana dels resultats de test per cada combinaci:\n",
      "102964.84631306672\n",
      "Les 20 caracterstiques ms importants sn:\n",
      "RPQ_PA: 0.1273\n",
      "RPQ01: 0.0861\n",
      "RPQ06: 0.0726\n",
      "RPQ_RA: 0.0706\n",
      "RPQ19: 0.0586\n",
      "RPQ12: 0.0544\n",
      "RPQ18: 0.0504\n",
      "RPQ16: 0.0464\n",
      "RPQ14: 0.0457\n",
      "RPQ21: 0.0425\n",
      "RPQ22: 0.0397\n",
      "RPQ03: 0.0388\n",
      "RPQ05: 0.0336\n",
      "RPQ08: 0.0254\n",
      "RPQ23: 0.0251\n",
      "RPQ04: 0.0246\n",
      "RPQ13: 0.0235\n",
      "RPQ11: 0.0199\n",
      "RPQ09: 0.0174\n",
      "RPQ17: 0.0170\n",
      "RPQ07: 0.0140\n",
      "RPQ15: 0.0138\n",
      "RPQ10: 0.0137\n",
      "RPQ20: 0.0135\n",
      "RPQ02: 0.0096\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100,250,500],\n",
    "    'max_depth': [30,50,75],\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    rf, \n",
    "    param_grid, \n",
    "    cv=10, \n",
    "    scoring='neg_mean_squared_error', \n",
    "    return_train_score=True\n",
    ")\n",
    "X_train_RPQ = filtered_datasets['RPQ']['X_train_filtrat']\n",
    "y_train_RPQ = filtered_datasets['RPQ']['y_train']\n",
    "\n",
    "# Realitzem el Grid Search\n",
    "grid_search.fit(X_train_RPQ, y_train_RPQ)\n",
    "\n",
    "# Resultats\n",
    "print(\"Millor parmetre:\", grid_search.best_params_)\n",
    "print(\"Millor precisi:\", -grid_search.best_score_)  # Negatiu perqu s'ha usat neg_mean_squared_error\n",
    "\n",
    "# Mostrem la mitjana dels resultats de test\n",
    "print(\"Mitjana dels resultats de test per cada combinaci:\")\n",
    "print(np.mean(grid_search.cv_results_['mean_test_score'] * -1))  # Multipliquem per -1 per obtenir el MSE positiu\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Obtenim la importncia de cada caracterstica\n",
    "coef_importants = best_model.feature_importances_\n",
    "\n",
    "# Obtenim els noms de les caracterstiques (suposant que tenim un DataFrame 'df')\n",
    "feature_names = X_train_RPQ.columns  \n",
    "\n",
    "# Ordenem les caracterstiques segons la seva importncia (de major a menor)\n",
    "indices_ordenats = coef_importants.argsort()[::-1][0:25]\n",
    "\n",
    "# Mostrem els noms de les 20 caracterstiques ms importants amb la seva importncia\n",
    "print(\"Les 20 caracterstiques ms importants sn:\")\n",
    "for i in indices_ordenats:\n",
    "    print(f\"{feature_names[i]}: {coef_importants[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ara farem CCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Millor parmetre: {'max_depth': 75, 'n_estimators': 100}\n",
      "Millor precisi: 96071.93678233135\n",
      "Mitjana dels resultats de test per cada combinaci:\n",
      "98755.98110486793\n",
      "Les 20 caracterstiques ms importants sn:\n",
      "CCA_PT: 0.1564\n",
      "CCA_CO: 0.1023\n",
      "CCA_DR: 0.0557\n",
      "CCA26: 0.0535\n",
      "CCA04: 0.0484\n",
      "CCA25: 0.0424\n",
      "CCA17: 0.0413\n",
      "CCA15: 0.0411\n",
      "CCA_CN: 0.0383\n",
      "CCA22: 0.0308\n",
      "CCA_VA: 0.0299\n",
      "CCA_RO: 0.0291\n",
      "CCA24: 0.0269\n",
      "CCA12: 0.0264\n",
      "CCA21: 0.0231\n",
      "CCA_AG: 0.0220\n",
      "CCA06: 0.0218\n",
      "CCA14: 0.0200\n",
      "CCA20: 0.0176\n",
      "CCA29: 0.0168\n",
      "CCA23: 0.0146\n",
      "CCA28: 0.0136\n",
      "CCA19: 0.0126\n",
      "CCA30: 0.0111\n",
      "CCA05: 0.0104\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100,250,500],\n",
    "    'max_depth': [30,50,75],\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    rf, \n",
    "    param_grid, \n",
    "    cv=10, \n",
    "    scoring='neg_mean_squared_error', \n",
    "    return_train_score=True\n",
    ")\n",
    "X_train_CCA = filtered_datasets['CCA']['X_train_filtrat']\n",
    "y_train_CCA = filtered_datasets['CCA']['y_train']\n",
    "\n",
    "# Realitzem el Grid Search\n",
    "grid_search.fit(X_train_CCA, y_train_CCA)\n",
    "\n",
    "# Resultats\n",
    "print(\"Millor parmetre:\", grid_search.best_params_)\n",
    "print(\"Millor precisi:\", -grid_search.best_score_)  # Negatiu perqu s'ha usat neg_mean_squared_error\n",
    "\n",
    "# Mostrem la mitjana dels resultats de test\n",
    "print(\"Mitjana dels resultats de test per cada combinaci:\")\n",
    "print(np.mean(grid_search.cv_results_['mean_test_score'] * -1))  # Multipliquem per -1 per obtenir el MSE positiu\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Obtenim la importncia de cada caracterstica\n",
    "coef_importants = best_model.feature_importances_\n",
    "\n",
    "# Obtenim els noms de les caracterstiques (suposant que tenim un DataFrame 'df')\n",
    "feature_names = X_train_CCA.columns  \n",
    "\n",
    "# Ordenem les caracterstiques segons la seva importncia (de major a menor)\n",
    "indices_ordenats = coef_importants.argsort()[::-1][0:25]\n",
    "\n",
    "# Mostrem els noms de les 20 caracterstiques ms importants amb la seva importncia\n",
    "print(\"Les 20 caracterstiques ms importants sn:\")\n",
    "for i in indices_ordenats:\n",
    "    print(f\"{feature_names[i]}: {coef_importants[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Farem el SD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Millor parmetre: {'max_depth': 30, 'n_estimators': 100}\n",
      "Millor precisi: 88597.49076028807\n",
      "Mitjana dels resultats de test per cada combinaci:\n",
      "91085.26934620566\n",
      "Les 20 caracterstiques ms importants sn:\n",
      "SD318: 0.1763\n",
      "SD313: 0.0883\n",
      "SD3Narc: 0.0566\n",
      "SD3Mach: 0.0528\n",
      "SD301: 0.0467\n",
      "SD316: 0.0454\n",
      "SD309: 0.0446\n",
      "SD325: 0.0424\n",
      "SD305: 0.0402\n",
      "xSD325: 0.0400\n",
      "SD324: 0.0346\n",
      "SD304: 0.0280\n",
      "SD314: 0.0268\n",
      "SD323: 0.0234\n",
      "SD321: 0.0218\n",
      "xSD320: 0.0197\n",
      "SD308: 0.0181\n",
      "SD306: 0.0171\n",
      "SD315: 0.0169\n",
      "SD3Mach_3gr: 0.0160\n",
      "SD322: 0.0155\n",
      "SD303: 0.0137\n",
      "SD320: 0.0118\n",
      "SD310: 0.0111\n",
      "SD326: 0.0105\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100,250,500],\n",
    "    'max_depth': [30,50,75],\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    rf, \n",
    "    param_grid, \n",
    "    cv=10, \n",
    "    scoring='neg_mean_squared_error', \n",
    "    return_train_score=True\n",
    ")\n",
    "X_train_SD3 = filtered_datasets['SD3']['X_train_filtrat']\n",
    "y_train_SD3 = filtered_datasets['SD3']['y_train']\n",
    "\n",
    "# Realitzem el Grid Search\n",
    "grid_search.fit(X_train_SD3, y_train_SD3)\n",
    "\n",
    "# Resultats\n",
    "print(\"Millor parmetre:\", grid_search.best_params_)\n",
    "print(\"Millor precisi:\", -grid_search.best_score_)  # Negatiu perqu s'ha usat neg_mean_squared_error\n",
    "\n",
    "# Mostrem la mitjana dels resultats de test\n",
    "print(\"Mitjana dels resultats de test per cada combinaci:\")\n",
    "print(np.mean(grid_search.cv_results_['mean_test_score'] * -1))  # Multipliquem per -1 per obtenir el MSE positiu\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Obtenim la importncia de cada caracterstica\n",
    "coef_importants = best_model.feature_importances_\n",
    "\n",
    "# Obtenim els noms de les caracterstiques (suposant que tenim un DataFrame 'df')\n",
    "feature_names = X_train_SD3.columns  \n",
    "\n",
    "# Ordenem les caracterstiques segons la seva importncia (de major a menor)\n",
    "indices_ordenats = coef_importants.argsort()[::-1][0:25]\n",
    "\n",
    "# Mostrem els noms de les 20 caracterstiques ms importants amb la seva importncia\n",
    "print(\"Les 20 caracterstiques ms importants sn:\")\n",
    "for i in indices_ordenats:\n",
    "    print(f\"{feature_names[i]}: {coef_importants[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Farem el ICUJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Millor parmetre: {'max_depth': 75, 'n_estimators': 100}\n",
      "Millor precisi: 103867.471216\n",
      "Mitjana dels resultats de test per cada combinaci:\n",
      "106314.23347673925\n",
      "Les 20 caracterstiques ms importants sn:\n",
      "xICUT15: 0.1003\n",
      "xICUT03: 0.0960\n",
      "ICUJ02: 0.0621\n",
      "xICUT24: 0.0547\n",
      "ICUJsp9ExtrmN: 0.0336\n",
      "xICUT19: 0.0320\n",
      "ICUJ18: 0.0305\n",
      "ICUJ24: 0.0249\n",
      "xICUT23: 0.0244\n",
      "ICUTsp8SplitN: 0.0227\n",
      "xICUT14: 0.0223\n",
      "xICUJ19: 0.0220\n",
      "ICUJ11: 0.0200\n",
      "ICUTsp4SplitN: 0.0197\n",
      "ICUTsp4ExtrmN: 0.0195\n",
      "ICUJ12: 0.0194\n",
      "ICUJ08: 0.0176\n",
      "ICUJ14: 0.0175\n",
      "ICUJ22: 0.0168\n",
      "ICUJ19: 0.0158\n",
      "ICUJ20: 0.0154\n",
      "ICUJsp4SplitN: 0.0144\n",
      "xICUJ01: 0.0137\n",
      "xICUJ13: 0.0119\n",
      "xICUJ24: 0.0113\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100,250,500],\n",
    "    'max_depth': [30,50,75],\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    rf, \n",
    "    param_grid, \n",
    "    cv=10, \n",
    "    scoring='neg_mean_squared_error', \n",
    "    return_train_score=True\n",
    ")\n",
    "X_train_ICUJ = filtered_datasets['ICUJ']['X_train_filtrat']\n",
    "y_train_ICUJ = filtered_datasets['ICUJ']['y_train']\n",
    "\n",
    "# Realitzem el Grid Search\n",
    "grid_search.fit(X_train_ICUJ, y_train_ICUJ)\n",
    "\n",
    "# Resultats\n",
    "print(\"Millor parmetre:\", grid_search.best_params_)\n",
    "print(\"Millor precisi:\", -grid_search.best_score_)  # Negatiu perqu s'ha usat neg_mean_squared_error\n",
    "\n",
    "# Mostrem la mitjana dels resultats de test\n",
    "print(\"Mitjana dels resultats de test per cada combinaci:\")\n",
    "print(np.mean(grid_search.cv_results_['mean_test_score'] * -1))  # Multipliquem per -1 per obtenir el MSE positiu\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Obtenim la importncia de cada caracterstica\n",
    "coef_importants = best_model.feature_importances_\n",
    "\n",
    "# Obtenim els noms de les caracterstiques (suposant que tenim un DataFrame 'df')\n",
    "feature_names = X_train_ICUJ.columns  \n",
    "\n",
    "# Ordenem les caracterstiques segons la seva importncia (de major a menor)\n",
    "indices_ordenats = coef_importants.argsort()[::-1][0:25]\n",
    "\n",
    "# Mostrem els noms de les 20 caracterstiques ms importants amb la seva importncia\n",
    "print(\"Les 20 caracterstiques ms importants sn:\")\n",
    "for i in indices_ordenats:\n",
    "    print(f\"{feature_names[i]}: {coef_importants[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Farem el ICUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Millor parmetre: {'max_depth': 75, 'n_estimators': 100}\n",
      "Millor precisi: 96806.91756633333\n",
      "Mitjana dels resultats de test per cada combinaci:\n",
      "98449.29728577925\n",
      "Les 20 caracterstiques ms importants sn:\n",
      "ICUT24: 0.1024\n",
      "ICUT15: 0.1021\n",
      "ICUT03: 0.0835\n",
      "ICUT_UC: 0.0734\n",
      "ICUT23: 0.0666\n",
      "ICUT21: 0.0454\n",
      "ICUT_CA: 0.0419\n",
      "ICUT14: 0.0411\n",
      "ICUT_PT: 0.0409\n",
      "ICUT05: 0.0357\n",
      "ICUT07: 0.0327\n",
      "ICUT19: 0.0318\n",
      "ICUT_UE: 0.0314\n",
      "ICUT20: 0.0283\n",
      "ICUT06: 0.0269\n",
      "ICUT22: 0.0219\n",
      "ICUT02: 0.0215\n",
      "ICUT17: 0.0213\n",
      "ICUT09: 0.0204\n",
      "ICUT13: 0.0204\n",
      "ICUT08: 0.0196\n",
      "ICUT12: 0.0162\n",
      "ICUT_PT_3gr: 0.0148\n",
      "ICUT16: 0.0131\n",
      "ICUT_PT_2gr: 0.0115\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100,250,500],\n",
    "    'max_depth': [30,50,75],\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    rf, \n",
    "    param_grid, \n",
    "    cv=10, \n",
    "    scoring='neg_mean_squared_error', \n",
    "    return_train_score=True\n",
    ")\n",
    "X_train_ICUT = filtered_datasets['ICUT']['X_train_filtrat']\n",
    "y_train_ICUT = filtered_datasets['ICUT']['y_train']\n",
    "\n",
    "# Realitzem el Grid Search\n",
    "grid_search.fit(X_train_ICUT, y_train_ICUT)\n",
    "\n",
    "# Resultats\n",
    "print(\"Millor parmetre:\", grid_search.best_params_)\n",
    "print(\"Millor precisi:\", -grid_search.best_score_)  # Negatiu perqu s'ha usat neg_mean_squared_error\n",
    "\n",
    "# Mostrem la mitjana dels resultats de test\n",
    "print(\"Mitjana dels resultats de test per cada combinaci:\")\n",
    "print(np.mean(grid_search.cv_results_['mean_test_score'] * -1))  # Multipliquem per -1 per obtenir el MSE positiu\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Obtenim la importncia de cada caracterstica\n",
    "coef_importants = best_model.feature_importances_\n",
    "\n",
    "# Obtenim els noms de les caracterstiques (suposant que tenim un DataFrame 'df')\n",
    "feature_names = X_train_ICUT.columns  \n",
    "\n",
    "# Ordenem les caracterstiques segons la seva importncia (de major a menor)\n",
    "indices_ordenats = coef_importants.argsort()[::-1][0:25]\n",
    "\n",
    "# Mostrem els noms de les 20 caracterstiques ms importants amb la seva importncia\n",
    "print(\"Les 20 caracterstiques ms importants sn:\")\n",
    "for i in indices_ordenats:\n",
    "    print(f\"{feature_names[i]}: {coef_importants[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculem el TRFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Millor parmetre: {'max_depth': 75, 'n_estimators': 100}\n",
      "Millor precisi: 83513.9029901776\n",
      "Mitjana dels resultats de test per cada combinaci:\n",
      "90497.30817980992\n",
      "Les 20 caracterstiques ms importants sn:\n",
      "TRFMSocProbT: 0.1092\n",
      "TRFMWithDepT: 0.0797\n",
      "TRFMSocProbCat: 0.0530\n",
      "TRFM049: 0.0502\n",
      "TRFMAtteProb: 0.0492\n",
      "TRFMIntProb: 0.0298\n",
      "TRFM003: 0.0294\n",
      "TRFMAnxDepT: 0.0288\n",
      "TRFM069: 0.0288\n",
      "TRFMDSMStrProb: 0.0262\n",
      "TRFMIntProbT: 0.0253\n",
      "TRFM042: 0.0243\n",
      "TRFM092: 0.0235\n",
      "TRFMDSMStrProbT: 0.0235\n",
      "TRFM050: 0.0219\n",
      "TRFMAtteProbT: 0.0209\n",
      "TRFMWithDepCat: 0.0194\n",
      "TRFMRulBehT: 0.0172\n",
      "TRFM096: 0.0163\n",
      "TRFM031: 0.0163\n",
      "TRFM035: 0.0157\n",
      "TRFM001: 0.0152\n",
      "TRFM072: 0.0150\n",
      "TRFM017: 0.0127\n",
      "TRFMAnxDepCat: 0.0123\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100,250,500],\n",
    "    'max_depth': [30,50,75],\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    rf, \n",
    "    param_grid, \n",
    "    cv=10, \n",
    "    scoring='neg_mean_squared_error', \n",
    "    return_train_score=True\n",
    ")\n",
    "X_train_TRFM = filtered_datasets['TRFM']['X_train_filtrat']\n",
    "y_train_TRFM = filtered_datasets['TRFM']['y_train']\n",
    "\n",
    "# Realitzem el Grid Search\n",
    "grid_search.fit(X_train_TRFM, y_train_TRFM)\n",
    "\n",
    "# Resultats\n",
    "print(\"Millor parmetre:\", grid_search.best_params_)\n",
    "print(\"Millor precisi:\", -grid_search.best_score_)  # Negatiu perqu s'ha usat neg_mean_squared_error\n",
    "\n",
    "# Mostrem la mitjana dels resultats de test\n",
    "print(\"Mitjana dels resultats de test per cada combinaci:\")\n",
    "print(np.mean(grid_search.cv_results_['mean_test_score'] * -1))  # Multipliquem per -1 per obtenir el MSE positiu\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Obtenim la importncia de cada caracterstica\n",
    "coef_importants = best_model.feature_importances_\n",
    "\n",
    "# Obtenim els noms de les caracterstiques (suposant que tenim un DataFrame 'df')\n",
    "feature_names = X_train_TRFM.columns  \n",
    "\n",
    "# Ordenem les caracterstiques segons la seva importncia (de major a menor)\n",
    "indices_ordenats = coef_importants.argsort()[::-1][0:25]\n",
    "\n",
    "# Mostrem els noms de les 20 caracterstiques ms importants amb la seva importncia\n",
    "print(\"Les 20 caracterstiques ms importants sn:\")\n",
    "for i in indices_ordenats:\n",
    "    print(f\"{feature_names[i]}: {coef_importants[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ara fem el dataset de TRFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Millor parmetre: {'max_depth': 75, 'n_estimators': 500}\n",
      "Millor precisi: 109493.12376245335\n",
      "Mitjana dels resultats de test per cada combinaci:\n",
      "113627.07636401037\n",
      "Les 20 caracterstiques ms importants sn:\n",
      "TRFT_OP: 0.0608\n",
      "TRFT_AP: 0.0577\n",
      "TRFT100: 0.0482\n",
      "TRFMDSMAnxProbCat: 0.0456\n",
      "TRFT084: 0.0352\n",
      "TRFTDSMStrProbT: 0.0261\n",
      "TRFMDSMAnxProbT: 0.0235\n",
      "TRFTDSMStrProb: 0.0234\n",
      "TRFTSocProbT: 0.0228\n",
      "TRFMDSMAnxProb: 0.0219\n",
      "TRFT077: 0.0210\n",
      "TRFTDSMCDT: 0.0191\n",
      "TRFT098: 0.0190\n",
      "TRFT081: 0.0190\n",
      "TRFTDSMCD: 0.0178\n",
      "TRFT109: 0.0166\n",
      "TRFT019: 0.0157\n",
      "TRFT092: 0.0153\n",
      "TRFTDSMObsComT: 0.0152\n",
      "TRFTExtProbT: 0.0138\n",
      "TRFTDSMDepProb: 0.0133\n",
      "TRFTDSMObsCom: 0.0129\n",
      "TRFTRulBehT: 0.0127\n",
      "TRFTDSMADHDT: 0.0126\n",
      "TRFTExtProb: 0.0126\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100,250,500],\n",
    "    'max_depth': [30,50,75],\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    rf, \n",
    "    param_grid, \n",
    "    cv=10, \n",
    "    scoring='neg_mean_squared_error', \n",
    "    return_train_score=True\n",
    ")\n",
    "X_train_TRFT = filtered_datasets['TRFT']['X_train_filtrat']\n",
    "y_train_TRFT = filtered_datasets['TRFT']['y_train']\n",
    "\n",
    "# Realitzem el Grid Search\n",
    "grid_search.fit(X_train_TRFT, y_train_TRFT)\n",
    "\n",
    "# Resultats\n",
    "print(\"Millor parmetre:\", grid_search.best_params_)\n",
    "print(\"Millor precisi:\", -grid_search.best_score_)  # Negatiu perqu s'ha usat neg_mean_squared_error\n",
    "\n",
    "# Mostrem la mitjana dels resultats de test\n",
    "print(\"Mitjana dels resultats de test per cada combinaci:\")\n",
    "print(np.mean(grid_search.cv_results_['mean_test_score'] * -1))  # Multipliquem per -1 per obtenir el MSE positiu\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Obtenim la importncia de cada caracterstica\n",
    "coef_importants = best_model.feature_importances_\n",
    "\n",
    "# Obtenim els noms de les caracterstiques (suposant que tenim un DataFrame 'df')\n",
    "feature_names = X_train_TRFT.columns  \n",
    "\n",
    "# Ordenem les caracterstiques segons la seva importncia (de major a menor)\n",
    "indices_ordenats = coef_importants.argsort()[::-1][0:25]\n",
    "\n",
    "# Mostrem els noms de les 20 caracterstiques ms importants amb la seva importncia\n",
    "print(\"Les 20 caracterstiques ms importants sn:\")\n",
    "for i in indices_ordenats:\n",
    "    print(f\"{feature_names[i]}: {coef_importants[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fem el dataset de YSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Millor parmetre: {'max_depth': 75, 'n_estimators': 500}\n",
      "Millor precisi: 74527.84699845333\n",
      "Mitjana dels resultats de test per cada combinaci:\n",
      "76506.03120982816\n",
      "Les 20 caracterstiques ms importants sn:\n",
      "YSR041: 0.1091\n",
      "YSRTotProb: 0.0686\n",
      "YSR077: 0.0509\n",
      "YSRTotProbT: 0.0501\n",
      "YSRDSMDepProb: 0.0444\n",
      "YSRDSMADHD: 0.0392\n",
      "TRFMAnxDep: 0.0390\n",
      "YSRSomComp: 0.0389\n",
      "YSRPosQualT: 0.0387\n",
      "YSRDSMCD: 0.0310\n",
      "YSRSomCompT: 0.0278\n",
      "YSRThouProb: 0.0230\n",
      "YSRExtProbT: 0.0209\n",
      "YSR076: 0.0200\n",
      "YSRDSMODD: 0.0197\n",
      "YSRThouProbT: 0.0177\n",
      "YSR090: 0.0174\n",
      "YSR006: 0.0170\n",
      "YSRIntProbT: 0.0148\n",
      "YSRDSMStrProb: 0.0138\n",
      "YSR047: 0.0134\n",
      "YSRExtProb: 0.0129\n",
      "YSRDSMAnxProbT: 0.0121\n",
      "YSR017: 0.0117\n",
      "YSR064: 0.0110\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100,250,500],\n",
    "    'max_depth': [30,50,75],\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    rf, \n",
    "    param_grid, \n",
    "    cv=10, \n",
    "    scoring='neg_mean_squared_error', \n",
    "    return_train_score=True\n",
    ")\n",
    "X_train_YSR = filtered_datasets['YSR']['X_train_filtrat']\n",
    "y_train_YSR = filtered_datasets['YSR']['y_train']\n",
    "\n",
    "# Realitzem el Grid Search\n",
    "grid_search.fit(X_train_YSR, y_train_YSR)\n",
    "\n",
    "# Resultats\n",
    "print(\"Millor parmetre:\", grid_search.best_params_)\n",
    "print(\"Millor precisi:\", -grid_search.best_score_)  # Negatiu perqu s'ha usat neg_mean_squared_error\n",
    "\n",
    "# Mostrem la mitjana dels resultats de test\n",
    "print(\"Mitjana dels resultats de test per cada combinaci:\")\n",
    "print(np.mean(grid_search.cv_results_['mean_test_score'] * -1))  # Multipliquem per -1 per obtenir el MSE positiu\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Obtenim la importncia de cada caracterstica\n",
    "coef_importants = best_model.feature_importances_\n",
    "\n",
    "# Obtenim els noms de les caracterstiques (suposant que tenim un DataFrame 'df')\n",
    "feature_names = X_train_YSR.columns  \n",
    "\n",
    "# Ordenem les caracterstiques segons la seva importncia (de major a menor)\n",
    "indices_ordenats = coef_importants.argsort()[::-1][0:25]\n",
    "\n",
    "# Mostrem els noms de les 20 caracterstiques ms importants amb la seva importncia\n",
    "print(\"Les 20 caracterstiques ms importants sn:\")\n",
    "for i in indices_ordenats:\n",
    "    print(f\"{feature_names[i]}: {coef_importants[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ara farem la triada fosca: Tri_PM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Millor parmetre: {'max_depth': 75, 'n_estimators': 600}\n",
      "Millor precisi: 72141.48493877778\n",
      "Mitjana dels resultats de test per cada combinaci:\n",
      "73308.12213091494\n",
      "Les 20 caracterstiques ms importants sn:\n",
      "TriPMTotal: 0.1123\n",
      "TriPM15: 0.0909\n",
      "TriPM51: 0.0747\n",
      "TriPM12: 0.0678\n",
      "TriPMDisi: 0.0637\n",
      "xTriPM47: 0.0421\n",
      "TriPM19: 0.0307\n",
      "TriPM50: 0.0249\n",
      "xTriPM50: 0.0244\n",
      "TriPMBold: 0.0222\n",
      "TriPM37: 0.0220\n",
      "TriPM55: 0.0166\n",
      "TriPM03: 0.0157\n",
      "TriPM40: 0.0145\n",
      "xTriPM41: 0.0141\n",
      "TriPM41: 0.0138\n",
      "TriPM34: 0.0131\n",
      "TriPM35: 0.0121\n",
      "TriPM57: 0.0119\n",
      "TriPM01: 0.0118\n",
      "TriPM06: 0.0117\n",
      "TriPM29: 0.0107\n",
      "TriPM20: 0.0107\n",
      "TriPM22: 0.0104\n",
      "TriPM07: 0.0102\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [500,600,750],\n",
    "    'max_depth': [75,90,100],\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    rf, \n",
    "    param_grid, \n",
    "    cv=10, \n",
    "    scoring='neg_mean_squared_error', \n",
    "    return_train_score=True\n",
    ")\n",
    "X_train_TriPM = filtered_datasets['TriPM']['X_train_filtrat']\n",
    "y_train_TriPM = filtered_datasets['TriPM']['y_train']\n",
    "\n",
    "# Realitzem el Grid Search\n",
    "grid_search.fit(X_train_TriPM, y_train_TriPM)\n",
    "\n",
    "# Resultats\n",
    "print(\"Millor parmetre:\", grid_search.best_params_)\n",
    "print(\"Millor precisi:\", -grid_search.best_score_)  # Negatiu perqu s'ha usat neg_mean_squared_error\n",
    "\n",
    "# Mostrem la mitjana dels resultats de test\n",
    "print(\"Mitjana dels resultats de test per cada combinaci:\")\n",
    "print(np.mean(grid_search.cv_results_['mean_test_score'] * -1))  # Multipliquem per -1 per obtenir el MSE positiu\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Obtenim la importncia de cada caracterstica\n",
    "coef_importants = best_model.feature_importances_\n",
    "\n",
    "# Obtenim els noms de les caracterstiques (suposant que tenim un DataFrame 'df')\n",
    "feature_names = X_train_TriPM.columns  \n",
    "\n",
    "# Ordenem les caracterstiques segons la seva importncia (de major a menor)\n",
    "indices_ordenats = coef_importants.argsort()[::-1][0:25]\n",
    "\n",
    "# Mostrem els noms de les 20 caracterstiques ms importants amb la seva importncia\n",
    "print(\"Les 20 caracterstiques ms importants sn:\")\n",
    "for i in indices_ordenats:\n",
    "    print(f\"{feature_names[i]}: {coef_importants[i]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
