{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crear_dataset import *\n",
    "from funcions_net import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyreadstat\n",
    "import pandas as pd\n",
    "from preprocessing import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from funcions_net import *\n",
    "from datasets_separats import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['temps_fins_reincidencia1a', 'YPI01', 'YPI02', 'YPI03', 'YPI04',\n",
      "       'YPI05', 'YPI06', 'YPI07', 'YPI08', 'YPI09', 'YPI10', 'YPI11', 'YPI12',\n",
      "       'YPI13', 'YPI14', 'YPI15', 'YPI16', 'YPI17', 'YPI18', 'YPI19', 'YPI20',\n",
      "       'YPI21', 'YPI22', 'YPI23', 'YPI24', 'YPI25', 'YPI26', 'YPI27', 'YPI28',\n",
      "       'YPI29', 'YPI30', 'YPI31', 'YPI32', 'YPI33', 'YPI34', 'YPI35', 'YPI36',\n",
      "       'YPI37', 'YPI38', 'YPI39', 'YPI40', 'YPI41', 'YPI42', 'YPI43', 'YPI44',\n",
      "       'YPI45', 'YPI46', 'YPI47', 'YPI48', 'YPI49', 'YPI50', 'xYPI23',\n",
      "       'xYPI35', 'xYPI49', 'YPI_DC', 'YPI_GR', 'YPI_LY', 'YPI_MA', 'YPI_RE',\n",
      "       'YPI_UN', 'YPI_CA', 'YPI_TS', 'YPI_IM', 'YPI_IR', 'YPI_PT', 'YPI_GM',\n",
      "       'YPI_CU', 'YPI_II', 'YPI_PT_3gr', 'YPI_GM_3gr', 'YPI_CU_3gr',\n",
      "       'YPI_II_3gr', 'YPI_PT_2gr', 'YPI_GM_2gr', 'YPI_CU_2gr', 'YPI_II_2gr'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\datasets_separats.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datasets[dataset_name][dict[i+1]] = df[dict[i+1]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_orig, meta = pyreadstat.read_sav(\"CEJFEAjut2015Updated.sav\")\n",
    "\n",
    "df_variables = pd.read_csv(\"variables.csv\", sep=';')\n",
    "\n",
    "dict = create_dict(meta)\n",
    "\n",
    "datasets = dataset_dif2(df_orig, dict,df_variables)\n",
    "\n",
    "print(datasets[\"YPI\"].columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\pipilenes.py:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_copy[col].fillna(X_copy[col].mean(), inplace=True)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\pipilenes.py:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_copy[col].fillna(X_copy[col].mean(), inplace=True)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\pipilenes.py:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_copy[col].fillna(X_copy[col].mean(), inplace=True)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\pipilenes.py:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_copy[col].fillna(X_copy[col].mean(), inplace=True)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\pipilenes.py:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_copy[col].fillna(X_copy[col].mean(), inplace=True)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\pipilenes.py:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_copy[col].fillna(X_copy[col].mean(), inplace=True)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\pipilenes.py:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_copy[col].fillna(X_copy[col].mean(), inplace=True)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\pipilenes.py:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_copy[col].fillna(X_copy[col].mean(), inplace=True)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\pipilenes.py:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_copy[col].fillna(X_copy[col].mean(), inplace=True)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\pipilenes.py:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_copy[col].fillna(X_copy[col].mean(), inplace=True)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\pipilenes.py:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_copy[col].fillna(X_copy[col].mean(), inplace=True)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\pipilenes.py:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_copy[col].fillna(X_copy[col].mean(), inplace=True)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\pipilenes.py:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_copy[col].fillna(X_copy[col].mean(), inplace=True)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\pipilenes.py:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_copy[col].fillna(X_copy[col].mean(), inplace=True)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\pipilenes.py:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_copy[col].fillna(X_copy[col].mean(), inplace=True)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\pipilenes.py:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_copy[col].fillna(X_copy[col].mean(), inplace=True)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\pipilenes.py:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_copy[col].fillna(X_copy[col].mean(), inplace=True)\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_test_splits = split_datasets(datasets)  # Apliquem la separació\n",
    "transformed_train_sets = apply_pipeline_to_train_sets(train_test_splits)  # Apliquem la pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RAVEN01</th>\n",
       "      <th>RAVEN02</th>\n",
       "      <th>RAVEN03</th>\n",
       "      <th>RAVEN04</th>\n",
       "      <th>RAVEN05</th>\n",
       "      <th>RAVEN_PD</th>\n",
       "      <th>RAVEN_Pc</th>\n",
       "      <th>RAVEN_IQ</th>\n",
       "      <th>RAVENbarem</th>\n",
       "      <th>RavenZ</th>\n",
       "      <th>Raven_IQ2</th>\n",
       "      <th>RavenZ_2</th>\n",
       "      <th>Raven_IQ3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-2.067064</td>\n",
       "      <td>68.994039</td>\n",
       "      <td>-2.178248</td>\n",
       "      <td>67.326284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.615499</td>\n",
       "      <td>109.232489</td>\n",
       "      <td>0.340872</td>\n",
       "      <td>105.113086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.168405</td>\n",
       "      <td>102.526080</td>\n",
       "      <td>0.443325</td>\n",
       "      <td>106.649874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-5.047690</td>\n",
       "      <td>24.284650</td>\n",
       "      <td>-3.964736</td>\n",
       "      <td>40.528967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>11.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.274637</td>\n",
       "      <td>80.880452</td>\n",
       "      <td>-1.274637</td>\n",
       "      <td>80.880452</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    RAVEN01  RAVEN02  RAVEN03  RAVEN04  RAVEN05  RAVEN_PD  RAVEN_Pc  RAVEN_IQ  \\\n",
       "31      7.0      7.0      5.0     10.0      3.0      32.0       5.0      76.0   \n",
       "23     10.0     10.0     11.0     10.0      9.0      50.0      75.0     110.0   \n",
       "58     12.0     12.0     11.0     11.0      1.0      47.0      50.0     100.0   \n",
       "17      6.0      1.0      2.0      2.0      1.0      12.0       3.0      70.0   \n",
       "69     11.0     10.0      8.0      9.0      2.0      40.0      50.0      90.0   \n",
       "\n",
       "    RAVENbarem    RavenZ   Raven_IQ2  RavenZ_2   Raven_IQ3  \n",
       "31         2.0 -2.067064   68.994039 -2.178248   67.326284  \n",
       "23         2.0  0.615499  109.232489  0.340872  105.113086  \n",
       "58         2.0  0.168405  102.526080  0.443325  106.649874  \n",
       "17         2.0 -5.047690   24.284650 -3.964736   40.528967  \n",
       "69         1.0 -1.274637   80.880452 -1.274637   80.880452  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_train_sets[\"IQ\"]['X_train'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'datasets_separats' from 'c:\\\\Users\\\\jjf20\\\\OneDrive\\\\Documents\\\\Universitat\\\\TFG\\\\datasets_separats.py'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets_separats\n",
    "import importlib\n",
    "importlib.reload(datasets_separats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrem a la funció:\n",
      "Applying significant features filter to dataset IQ...\n",
      "Tamanys train (57, 13)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 13)\n",
      "Tamanys train (57, 13)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 13)\n",
      "Applying significant features filter to dataset SAVRY...\n",
      "Tamanys train (57, 42)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 42)\n",
      "Tamanys train (57, 42)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 42)\n",
      "Applying significant features filter to dataset VAST...\n",
      "Tamanys train (57, 23)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 23)\n",
      "Tamanys train (57, 23)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 23)\n",
      "Applying significant features filter to dataset PCL...\n",
      "Tamanys train (57, 141)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 141)\n",
      "Tamanys train (57, 141)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 141)\n",
      "Applying significant features filter to dataset CAPE...\n",
      "Tamanys train (57, 17)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 17)\n",
      "Tamanys train (57, 17)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 17)\n",
      "Applying significant features filter to dataset YPI...\n",
      "Tamanys train (57, 75)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 75)\n",
      "Tamanys train (57, 75)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 75)\n",
      "Applying significant features filter to dataset RPQ...\n",
      "Tamanys train (57, 29)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 29)\n",
      "Tamanys train (57, 29)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 29)\n",
      "Applying significant features filter to dataset CCA...\n",
      "Tamanys train (57, 40)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 40)\n",
      "Tamanys train (57, 40)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 40)\n",
      "Applying significant features filter to dataset SD3...\n",
      "Tamanys train (57, 39)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 39)\n",
      "Tamanys train (57, 39)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 39)\n",
      "Applying significant features filter to dataset ICUJ...\n",
      "Tamanys train (57, 78)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 78)\n",
      "Tamanys train (57, 78)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 78)\n",
      "Applying significant features filter to dataset ICUT...\n",
      "Tamanys train (57, 30)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 30)\n",
      "Tamanys train (57, 30)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 30)\n",
      "Applying significant features filter to dataset TRFM...\n",
      "Tamanys train (57, 173)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 173)\n",
      "Tamanys train (57, 173)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 173)\n",
      "Applying significant features filter to dataset YSR...\n",
      "Tamanys train (57, 181)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 181)\n",
      "Tamanys train (57, 181)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 181)\n",
      "Applying significant features filter to dataset TriPM...\n",
      "Tamanys train (57, 87)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 87)\n",
      "Tamanys train (57, 87)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 87)\n",
      "Applying significant features filter to dataset TRFT...\n",
      "Tamanys train (57, 197)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 197)\n",
      "Tamanys train (57, 197)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 197)\n",
      "Applying significant features filter to dataset DD...\n",
      "Tamanys train (57, 24)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 24)\n",
      "Tamanys train (57, 24)\n",
      "Tamanys train y (57,)\n",
      "Tamanys train test (15, 24)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.923e+06, tolerance: 4.510e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.296e+05, tolerance: 4.510e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.243e+05, tolerance: 4.510e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.660e+04, tolerance: 4.510e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.063e+05, tolerance: 4.510e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.653e+04, tolerance: 4.510e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.152e+04, tolerance: 4.510e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.853e+05, tolerance: 4.510e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.500e+04, tolerance: 4.510e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.577e+04, tolerance: 4.510e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.257e+04, tolerance: 4.510e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\jjf20\\OneDrive\\Documents\\Universitat\\TFG\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.695e+04, tolerance: 4.510e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "filtered_datasets = datasets_separats.apply_significant_features_filter(transformed_train_sets, df_orig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Millor paràmetre: {'max_depth': 50, 'n_estimators': 250}\n",
      "Millor precisió: 101505.30808600248\n",
      "Mitjana dels resultats de test per cada combinació:\n",
      "103494.1985661582\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "## fem grid search per trobar els millors hiperparàmetres\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100,250,500],\n",
    "    'max_depth': [30,50,75],\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    rf, \n",
    "    param_grid, \n",
    "    cv=10, \n",
    "    scoring='neg_mean_squared_error', \n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "X_train_IQ = filtered_datasets['IQ']['X_train_filtrat']\n",
    "y_train_IQ = filtered_datasets['IQ']['y_train']\n",
    "\n",
    "# Realitzem el Grid Search\n",
    "grid_search.fit(X_train_IQ, y_train_IQ)\n",
    "\n",
    "# Resultats\n",
    "print(\"Millor paràmetre:\", grid_search.best_params_)\n",
    "print(\"Millor precisió:\", -grid_search.best_score_)  # Negatiu perquè s'ha usat neg_mean_squared_error\n",
    "\n",
    "# Mostrem la mitjana dels resultats de test\n",
    "print(\"Mitjana dels resultats de test per cada combinació:\")\n",
    "print(np.mean(grid_search.cv_results_['mean_test_score'] * -1))  # Multipliquem per -1 per obtenir el MSE positiu\n",
    "\n",
    "best_model = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les 20 característiques més importants són:\n",
      "RAVEN05: 0.1733\n",
      "RavenZ_2: 0.1391\n",
      "RAVEN04: 0.1279\n",
      "Raven_IQ3: 0.1101\n",
      "RAVEN03: 0.0836\n",
      "RAVEN_PD: 0.0783\n",
      "RAVEN02: 0.0684\n",
      "Raven_IQ2: 0.0657\n",
      "RavenZ: 0.0544\n",
      "RAVEN01: 0.0411\n",
      "RAVEN_IQ: 0.0247\n",
      "RAVEN_Pc: 0.0244\n",
      "RAVENbarem: 0.0090\n"
     ]
    }
   ],
   "source": [
    "# Obtenim la importància de cada característica\n",
    "coef_importants = best_model.feature_importances_\n",
    "\n",
    "# Obtenim els noms de les característiques (suposant que tenim un DataFrame 'df')\n",
    "feature_names = X_train_IQ.columns  \n",
    "\n",
    "# Ordenem les característiques segons la seva importància (de major a menor)\n",
    "indices_ordenats = coef_importants.argsort()[::-1][0:25]\n",
    "\n",
    "# Mostrem els noms de les 20 característiques més importants amb la seva importància\n",
    "print(\"Les 20 característiques més importants són:\")\n",
    "for i in indices_ordenats:\n",
    "    print(f\"{feature_names[i]}: {coef_importants[i]:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les millors característiques son RAVEN05 i RAVEN02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ara comencem amb el segon dataset que és SAVRY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Millor paràmetre: {'max_depth': 20, 'n_estimators': 900}\n",
      "Millor precisió: 76330.16659849383\n",
      "Mitjana dels resultats de test per cada combinació:\n",
      "77586.38044545265\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [500,700,900],\n",
    "    'max_depth': [10,20,30],\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    rf, \n",
    "    param_grid, \n",
    "    cv=10, \n",
    "    scoring='neg_mean_squared_error', \n",
    "    return_train_score=True\n",
    ")\n",
    "X_train_SAVRY = filtered_datasets['SAVRY']['X_train_filtrat']\n",
    "y_train_SAVRY = filtered_datasets['SAVRY']['y_train']\n",
    "\n",
    "# Realitzem el Grid Search\n",
    "grid_search.fit(X_train_SAVRY, y_train_SAVRY)\n",
    "\n",
    "# Resultats\n",
    "print(\"Millor paràmetre:\", grid_search.best_params_)\n",
    "print(\"Millor precisió:\", -grid_search.best_score_)  # Negatiu perquè s'ha usat neg_mean_squared_error\n",
    "\n",
    "# Mostrem la mitjana dels resultats de test\n",
    "print(\"Mitjana dels resultats de test per cada combinació:\")\n",
    "print(np.mean(grid_search.cv_results_['mean_test_score'] * -1))  # Multipliquem per -1 per obtenir el MSE positiu\n",
    "\n",
    "best_model = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les 20 característiques més importants són:\n",
      "SAVRY02: 0.1870\n",
      "SAVRYSoc: 0.0786\n",
      "SAVRY11: 0.0695\n",
      "SAVRY05: 0.0685\n",
      "SAVRYTot: 0.0626\n",
      "SAVRY25: 0.0558\n",
      "SAVRYHis: 0.0454\n",
      "SAVRY04: 0.0419\n",
      "SAVRYInd: 0.0366\n",
      "SAVRY36: 0.0296\n",
      "SAVRY38: 0.0233\n",
      "SAVRY29: 0.0202\n",
      "SAVRY08: 0.0175\n",
      "SAVRY01: 0.0172\n",
      "SAVRY28: 0.0157\n",
      "SAVRY20: 0.0154\n",
      "SAVRY30: 0.0153\n",
      "SAVRY27: 0.0150\n",
      "SAVRY09: 0.0148\n",
      "SAVRY18: 0.0146\n",
      "SAVRY13: 0.0137\n",
      "SAVRY16: 0.0121\n",
      "SAVRY15: 0.0113\n",
      "SAVRY39: 0.0107\n",
      "SAVRY21: 0.0105\n"
     ]
    }
   ],
   "source": [
    "# Obtenim la importància de cada característica\n",
    "coef_importants = best_model.feature_importances_\n",
    "\n",
    "# Obtenim els noms de les característiques (suposant que tenim un DataFrame 'df')\n",
    "feature_names = X_train_SAVRY.columns  \n",
    "\n",
    "# Ordenem les característiques segons la seva importància (de major a menor)\n",
    "indices_ordenats = coef_importants.argsort()[::-1][0:25]\n",
    "\n",
    "# Mostrem els noms de les 20 característiques més importants amb la seva importància\n",
    "print(\"Les 20 característiques més importants són:\")\n",
    "for i in indices_ordenats:\n",
    "    print(f\"{feature_names[i]}: {coef_importants[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les millos caractqristiques\n",
    "SAVRYHis: 0.1155\n",
    "SAVRY05: 0.1019\n",
    "SAVRYTot: 0.0970"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fem ara el model DD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Millor paràmetre: {'max_depth': 30, 'n_estimators': 500}\n",
      "Millor precisió: 110740.10380762618\n",
      "Mitjana dels resultats de test per cada combinació:\n",
      "112934.70358308073\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [500,700,900],\n",
    "    'max_depth': [10,20,30],\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    rf, \n",
    "    param_grid, \n",
    "    cv=10, \n",
    "    scoring='neg_mean_squared_error', \n",
    "    return_train_score=True\n",
    ")\n",
    "X_train_DD = filtered_datasets['DD']['X_train_filtrat']\n",
    "y_train_DD = filtered_datasets['DD']['y_train']\n",
    "\n",
    "# Realitzem el Grid Search\n",
    "grid_search.fit(X_train_DD, y_train_DD)\n",
    "\n",
    "# Resultats\n",
    "print(\"Millor paràmetre:\", grid_search.best_params_)\n",
    "print(\"Millor precisió:\", -grid_search.best_score_)  # Negatiu perquè s'ha usat neg_mean_squared_error\n",
    "\n",
    "# Mostrem la mitjana dels resultats de test\n",
    "print(\"Mitjana dels resultats de test per cada combinació:\")\n",
    "print(np.mean(grid_search.cv_results_['mean_test_score'] * -1))  # Multipliquem per -1 per obtenir el MSE positiu\n",
    "\n",
    "best_model = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les 20 característiques més importants són:\n",
      "DD02: 0.1350\n",
      "DD08: 0.1161\n",
      "DD05: 0.0880\n",
      "DDNarc: 0.0858\n",
      "DDPsyc: 0.0638\n",
      "DD03: 0.0619\n",
      "DD07: 0.0607\n",
      "DD04: 0.0575\n",
      "DD01: 0.0537\n",
      "DDMach: 0.0493\n",
      "DD09: 0.0449\n",
      "DD06: 0.0363\n",
      "DDTotal_3gr: 0.0344\n",
      "DD12: 0.0256\n",
      "DDPsyc_3gr: 0.0198\n",
      "DDMach_3gr: 0.0192\n",
      "DDNarc_3gr: 0.0184\n",
      "DDPsyc_2gr: 0.0084\n",
      "DDMach_2gr: 0.0082\n",
      "DDTotal_2gr: 0.0078\n",
      "DDNarc_2gr: 0.0054\n"
     ]
    }
   ],
   "source": [
    "# Obtenim la importància de cada característica\n",
    "coef_importants = best_model.feature_importances_\n",
    "\n",
    "# Obtenim els noms de les característiques (suposant que tenim un DataFrame 'df')\n",
    "feature_names = X_train_DD.columns  \n",
    "\n",
    "# Ordenem les característiques segons la seva importància (de major a menor)\n",
    "indices_ordenats = coef_importants.argsort()[::-1][0:25]\n",
    "\n",
    "# Mostrem els noms de les 20 característiques més importants amb la seva importància\n",
    "print(\"Les 20 característiques més importants són:\")\n",
    "for i in indices_ordenats:\n",
    "    print(f\"{feature_names[i]}: {coef_importants[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ara farem VAST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Millor paràmetre: {'max_depth': 15, 'n_estimators': 100}\n",
      "Millor precisió: 91057.3631693837\n",
      "Mitjana dels resultats de test per cada combinació:\n",
      "96837.61777334922\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [50,100,200],\n",
    "    'max_depth': [15,30,45],\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    rf, \n",
    "    param_grid, \n",
    "    cv=10, \n",
    "    scoring='neg_mean_squared_error', \n",
    "    return_train_score=True\n",
    ")\n",
    "X_train_VAST = filtered_datasets['VAST']['X_train_filtrat']\n",
    "y_train_VAST = filtered_datasets['VAST']['y_train']\n",
    "\n",
    "# Realitzem el Grid Search\n",
    "grid_search.fit(X_train_VAST, y_train_VAST)\n",
    "\n",
    "# Resultats\n",
    "print(\"Millor paràmetre:\", grid_search.best_params_)\n",
    "print(\"Millor precisió:\", -grid_search.best_score_)  # Negatiu perquè s'ha usat neg_mean_squared_error\n",
    "\n",
    "# Mostrem la mitjana dels resultats de test\n",
    "print(\"Mitjana dels resultats de test per cada combinació:\")\n",
    "print(np.mean(grid_search.cv_results_['mean_test_score'] * -1))  # Multipliquem per -1 per obtenir el MSE positiu\n",
    "\n",
    "best_model = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les 20 característiques més importants són:\n",
      "VAST18: 0.0969\n",
      "VAST14: 0.0943\n",
      "VASTDir: 0.0894\n",
      "VAST10: 0.0794\n",
      "VAST17: 0.0751\n",
      "VAST12: 0.0724\n",
      "VAST01: 0.0647\n",
      "VAST06: 0.0543\n",
      "VAST02: 0.0467\n",
      "VASTVic: 0.0423\n",
      "VASTTotal: 0.0414\n",
      "VAST19: 0.0370\n",
      "VAST08: 0.0362\n",
      "VAST03: 0.0304\n",
      "VAST11: 0.0233\n",
      "VAST13: 0.0195\n",
      "VAST05: 0.0181\n",
      "VAST04: 0.0177\n",
      "VAST09: 0.0154\n",
      "VAST15: 0.0148\n",
      "VAST20: 0.0120\n",
      "VAST07: 0.0102\n",
      "VAST16: 0.0084\n"
     ]
    }
   ],
   "source": [
    "# Obtenim la importància de cada característica\n",
    "coef_importants = best_model.feature_importances_\n",
    "\n",
    "# Obtenim els noms de les característiques (suposant que tenim un DataFrame 'df')\n",
    "feature_names = X_train_VAST.columns  \n",
    "\n",
    "# Ordenem les característiques segons la seva importància (de major a menor)\n",
    "indices_ordenats = coef_importants.argsort()[::-1][0:25]\n",
    "\n",
    "# Mostrem els noms de les 20 característiques més importants amb la seva importància\n",
    "print(\"Les 20 característiques més importants són:\")\n",
    "for i in indices_ordenats:\n",
    "    print(f\"{feature_names[i]}: {coef_importants[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fem el dataset de PCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Millor paràmetre: {'max_depth': 30, 'n_estimators': 25}\n",
      "Millor precisió: 64323.10539812003\n",
      "Mitjana dels resultats de test per cada combinació:\n",
      "74157.65885676327\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [25,50,100],\n",
    "    'max_depth': [30,50,75],\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    rf, \n",
    "    param_grid, \n",
    "    cv=10, \n",
    "    scoring='neg_mean_squared_error', \n",
    "    return_train_score=True\n",
    ")\n",
    "X_train_PCL = filtered_datasets['PCL']['X_train_filtrat']\n",
    "y_train_PCL = filtered_datasets['PCL']['y_train']\n",
    "\n",
    "# Realitzem el Grid Search\n",
    "grid_search.fit(X_train_PCL, y_train_PCL)\n",
    "\n",
    "# Resultats\n",
    "print(\"Millor paràmetre:\", grid_search.best_params_)\n",
    "print(\"Millor precisió:\", -grid_search.best_score_)  # Negatiu perquè s'ha usat neg_mean_squared_error\n",
    "\n",
    "# Mostrem la mitjana dels resultats de test\n",
    "print(\"Mitjana dels resultats de test per cada combinació:\")\n",
    "print(np.mean(grid_search.cv_results_['mean_test_score'] * -1))  # Multipliquem per -1 per obtenir el MSE positiu\n",
    "\n",
    "best_model = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les 20 característiques més importants són:\n",
      "PCLjFAC_3: 0.3509\n",
      "PCLj14: 0.0528\n",
      "PCLjFAC_1: 0.0495\n",
      "PCLjFAC_2: 0.0483\n",
      "PCLeFAC_2: 0.0381\n",
      "PCLj12: 0.0343\n",
      "PCLoFAC_1: 0.0285\n",
      "PCLo13: 0.0276\n",
      "PCLj11: 0.0252\n",
      "PCLo10: 0.0183\n",
      "PCLo_PT: 0.0181\n",
      "PCLo05: 0.0147\n",
      "PCLx09: 0.0139\n",
      "PCLe16: 0.0138\n",
      "PCLe_PT: 0.0123\n",
      "PCLe08: 0.0122\n",
      "PCLeFAC_3: 0.0119\n",
      "PCLYVMPT: 0.0112\n",
      "PCLx16: 0.0112\n",
      "PCLo09: 0.0110\n",
      "PCLxFAC_1: 0.0106\n",
      "PCLYVMFAC_4_3gr: 0.0103\n",
      "PCLxFAC_3: 0.0097\n",
      "PCLoFAC_2: 0.0094\n",
      "PCLoFAC_4: 0.0077\n"
     ]
    }
   ],
   "source": [
    "# Obtenim la importància de cada característica\n",
    "coef_importants = best_model.feature_importances_\n",
    "\n",
    "# Obtenim els noms de les característiques (suposant que tenim un DataFrame 'df')\n",
    "feature_names = X_train_PCL.columns  \n",
    "\n",
    "# Ordenem les característiques segons la seva importància (de major a menor)\n",
    "indices_ordenats = coef_importants.argsort()[::-1][0:25]\n",
    "\n",
    "# Mostrem els noms de les 20 característiques més importants amb la seva importància\n",
    "print(\"Les 20 característiques més importants són:\")\n",
    "for i in indices_ordenats:\n",
    "    print(f\"{feature_names[i]}: {coef_importants[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claus amb valor PCLjFAC_3: [258]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "valor_que_busco = \"PCLjFAC_3\"\n",
    "\n",
    "claus = [clau for clau, valor in dict.items() if valor == valor_que_busco]\n",
    "\n",
    "print(f\"Claus amb valor {valor_que_busco}: {claus}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ara anem a fer CAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Millor paràmetre: {'max_depth': 75, 'n_estimators': 100}\n",
      "Millor precisió: 77951.54423988292\n",
      "Mitjana dels resultats de test per cada combinació:\n",
      "80758.97468973516\n",
      "Les 20 característiques més importants són:\n",
      "CAPEe03: 0.4243\n",
      "CAPEe04: 0.0897\n",
      "CAPEe01: 0.0803\n",
      "CAPEo201: 0.0606\n",
      "CAPEo01: 0.0568\n",
      "CAPEo202: 0.0509\n",
      "CAPEo203: 0.0386\n",
      "CAPEo02: 0.0374\n",
      "CAPEo04: 0.0356\n",
      "CAPEo03: 0.0283\n",
      "CAPEe02: 0.0264\n",
      "CAPEe_PT_2gr: 0.0163\n",
      "CAPEo_PT_3gr: 0.0159\n",
      "CAPEo204: 0.0156\n",
      "CAPEo_PT_2gr: 0.0135\n",
      "CAPEe_PT_3gr: 0.0098\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100,250,500],\n",
    "    'max_depth': [30,50,75],\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    rf, \n",
    "    param_grid, \n",
    "    cv=10, \n",
    "    scoring='neg_mean_squared_error', \n",
    "    return_train_score=True\n",
    ")\n",
    "X_train_CAPE = filtered_datasets['CAPE']['X_train_filtrat']\n",
    "y_train_CAPE = filtered_datasets['CAPE']['y_train']\n",
    "\n",
    "# Realitzem el Grid Search\n",
    "grid_search.fit(X_train_CAPE, y_train_CAPE)\n",
    "\n",
    "# Resultats\n",
    "print(\"Millor paràmetre:\", grid_search.best_params_)\n",
    "print(\"Millor precisió:\", -grid_search.best_score_)  # Negatiu perquè s'ha usat neg_mean_squared_error\n",
    "\n",
    "# Mostrem la mitjana dels resultats de test\n",
    "print(\"Mitjana dels resultats de test per cada combinació:\")\n",
    "print(np.mean(grid_search.cv_results_['mean_test_score'] * -1))  # Multipliquem per -1 per obtenir el MSE positiu\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Obtenim la importància de cada característica\n",
    "coef_importants = best_model.feature_importances_\n",
    "\n",
    "# Obtenim els noms de les característiques (suposant que tenim un DataFrame 'df')\n",
    "feature_names = X_train_CAPE.columns  \n",
    "\n",
    "# Ordenem les característiques segons la seva importància (de major a menor)\n",
    "indices_ordenats = coef_importants.argsort()[::-1][0:25]\n",
    "\n",
    "# Mostrem els noms de les 20 característiques més importants amb la seva importància\n",
    "print(\"Les 20 característiques més importants són:\")\n",
    "for i in indices_ordenats:\n",
    "    print(f\"{feature_names[i]}: {coef_importants[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ara anem a fer YPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Millor paràmetre: {'max_depth': 20, 'n_estimators': 100}\n",
      "Millor precisió: 78064.09418793334\n",
      "Mitjana dels resultats de test per cada combinació:\n",
      "79980.0777389698\n",
      "Les 20 característiques més importants són:\n",
      "YPI32: 0.0814\n",
      "YPI24: 0.0772\n",
      "YPI05: 0.0762\n",
      "YPI09: 0.0554\n",
      "YPI33: 0.0482\n",
      "YPI04: 0.0386\n",
      "YPI13: 0.0310\n",
      "YPI10: 0.0280\n",
      "YPI15: 0.0270\n",
      "YPI39: 0.0253\n",
      "xYPI35: 0.0203\n",
      "YPI25: 0.0191\n",
      "YPI46: 0.0190\n",
      "YPI11: 0.0190\n",
      "YPI17: 0.0190\n",
      "YPI47: 0.0184\n",
      "YPI07: 0.0183\n",
      "YPI_CU_3gr: 0.0178\n",
      "YPI29: 0.0168\n",
      "YPI03: 0.0167\n",
      "YPI36: 0.0161\n",
      "YPI_GM_3gr: 0.0156\n",
      "YPI42: 0.0155\n",
      "YPI49: 0.0135\n",
      "YPI50: 0.0134\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100,250,500],\n",
    "    'max_depth': [20,30,50],\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    rf, \n",
    "    param_grid, \n",
    "    cv=10, \n",
    "    scoring='neg_mean_squared_error', \n",
    "    return_train_score=True\n",
    ")\n",
    "X_train_YPI = filtered_datasets['YPI']['X_train_filtrat']\n",
    "y_train_YPI = filtered_datasets['YPI']['y_train']\n",
    "\n",
    "# Realitzem el Grid Search\n",
    "grid_search.fit(X_train_YPI, y_train_YPI)\n",
    "\n",
    "# Resultats\n",
    "print(\"Millor paràmetre:\", grid_search.best_params_)\n",
    "print(\"Millor precisió:\", -grid_search.best_score_)  # Negatiu perquè s'ha usat neg_mean_squared_error\n",
    "\n",
    "# Mostrem la mitjana dels resultats de test\n",
    "print(\"Mitjana dels resultats de test per cada combinació:\")\n",
    "print(np.mean(grid_search.cv_results_['mean_test_score'] * -1))  # Multipliquem per -1 per obtenir el MSE positiu\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Obtenim la importància de cada característica\n",
    "coef_importants = best_model.feature_importances_\n",
    "\n",
    "# Obtenim els noms de les característiques (suposant que tenim un DataFrame 'df')\n",
    "feature_names = X_train_YPI.columns  \n",
    "\n",
    "# Ordenem les característiques segons la seva importància (de major a menor)\n",
    "indices_ordenats = coef_importants.argsort()[::-1][0:25]\n",
    "\n",
    "# Mostrem els noms de les 20 característiques més importants amb la seva importància\n",
    "print(\"Les 20 característiques més importants són:\")\n",
    "for i in indices_ordenats:\n",
    "    print(f\"{feature_names[i]}: {coef_importants[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Farem servir servir el dataset RPQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Millor paràmetre: {'max_depth': 75, 'n_estimators': 500}\n",
      "Millor precisió: 100724.08727673178\n",
      "Mitjana dels resultats de test per cada combinació:\n",
      "102964.84631306672\n",
      "Les 20 característiques més importants són:\n",
      "RPQ_PA: 0.1273\n",
      "RPQ01: 0.0861\n",
      "RPQ06: 0.0726\n",
      "RPQ_RA: 0.0706\n",
      "RPQ19: 0.0586\n",
      "RPQ12: 0.0544\n",
      "RPQ18: 0.0504\n",
      "RPQ16: 0.0464\n",
      "RPQ14: 0.0457\n",
      "RPQ21: 0.0425\n",
      "RPQ22: 0.0397\n",
      "RPQ03: 0.0388\n",
      "RPQ05: 0.0336\n",
      "RPQ08: 0.0254\n",
      "RPQ23: 0.0251\n",
      "RPQ04: 0.0246\n",
      "RPQ13: 0.0235\n",
      "RPQ11: 0.0199\n",
      "RPQ09: 0.0174\n",
      "RPQ17: 0.0170\n",
      "RPQ07: 0.0140\n",
      "RPQ15: 0.0138\n",
      "RPQ10: 0.0137\n",
      "RPQ20: 0.0135\n",
      "RPQ02: 0.0096\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100,250,500],\n",
    "    'max_depth': [30,50,75],\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    rf, \n",
    "    param_grid, \n",
    "    cv=10, \n",
    "    scoring='neg_mean_squared_error', \n",
    "    return_train_score=True\n",
    ")\n",
    "X_train_RPQ = filtered_datasets['RPQ']['X_train_filtrat']\n",
    "y_train_RPQ = filtered_datasets['RPQ']['y_train']\n",
    "\n",
    "# Realitzem el Grid Search\n",
    "grid_search.fit(X_train_RPQ, y_train_RPQ)\n",
    "\n",
    "# Resultats\n",
    "print(\"Millor paràmetre:\", grid_search.best_params_)\n",
    "print(\"Millor precisió:\", -grid_search.best_score_)  # Negatiu perquè s'ha usat neg_mean_squared_error\n",
    "\n",
    "# Mostrem la mitjana dels resultats de test\n",
    "print(\"Mitjana dels resultats de test per cada combinació:\")\n",
    "print(np.mean(grid_search.cv_results_['mean_test_score'] * -1))  # Multipliquem per -1 per obtenir el MSE positiu\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Obtenim la importància de cada característica\n",
    "coef_importants = best_model.feature_importances_\n",
    "\n",
    "# Obtenim els noms de les característiques (suposant que tenim un DataFrame 'df')\n",
    "feature_names = X_train_RPQ.columns  \n",
    "\n",
    "# Ordenem les característiques segons la seva importància (de major a menor)\n",
    "indices_ordenats = coef_importants.argsort()[::-1][0:25]\n",
    "\n",
    "# Mostrem els noms de les 20 característiques més importants amb la seva importància\n",
    "print(\"Les 20 característiques més importants són:\")\n",
    "for i in indices_ordenats:\n",
    "    print(f\"{feature_names[i]}: {coef_importants[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ara farem CCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Millor paràmetre: {'max_depth': 75, 'n_estimators': 100}\n",
      "Millor precisió: 96071.93678233135\n",
      "Mitjana dels resultats de test per cada combinació:\n",
      "98755.98110486793\n",
      "Les 20 característiques més importants són:\n",
      "CCA_PT: 0.1564\n",
      "CCA_CO: 0.1023\n",
      "CCA_DR: 0.0557\n",
      "CCA26: 0.0535\n",
      "CCA04: 0.0484\n",
      "CCA25: 0.0424\n",
      "CCA17: 0.0413\n",
      "CCA15: 0.0411\n",
      "CCA_CN: 0.0383\n",
      "CCA22: 0.0308\n",
      "CCA_VA: 0.0299\n",
      "CCA_RO: 0.0291\n",
      "CCA24: 0.0269\n",
      "CCA12: 0.0264\n",
      "CCA21: 0.0231\n",
      "CCA_AG: 0.0220\n",
      "CCA06: 0.0218\n",
      "CCA14: 0.0200\n",
      "CCA20: 0.0176\n",
      "CCA29: 0.0168\n",
      "CCA23: 0.0146\n",
      "CCA28: 0.0136\n",
      "CCA19: 0.0126\n",
      "CCA30: 0.0111\n",
      "CCA05: 0.0104\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100,250,500],\n",
    "    'max_depth': [30,50,75],\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    rf, \n",
    "    param_grid, \n",
    "    cv=10, \n",
    "    scoring='neg_mean_squared_error', \n",
    "    return_train_score=True\n",
    ")\n",
    "X_train_CCA = filtered_datasets['CCA']['X_train_filtrat']\n",
    "y_train_CCA = filtered_datasets['CCA']['y_train']\n",
    "\n",
    "# Realitzem el Grid Search\n",
    "grid_search.fit(X_train_CCA, y_train_CCA)\n",
    "\n",
    "# Resultats\n",
    "print(\"Millor paràmetre:\", grid_search.best_params_)\n",
    "print(\"Millor precisió:\", -grid_search.best_score_)  # Negatiu perquè s'ha usat neg_mean_squared_error\n",
    "\n",
    "# Mostrem la mitjana dels resultats de test\n",
    "print(\"Mitjana dels resultats de test per cada combinació:\")\n",
    "print(np.mean(grid_search.cv_results_['mean_test_score'] * -1))  # Multipliquem per -1 per obtenir el MSE positiu\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Obtenim la importància de cada característica\n",
    "coef_importants = best_model.feature_importances_\n",
    "\n",
    "# Obtenim els noms de les característiques (suposant que tenim un DataFrame 'df')\n",
    "feature_names = X_train_CCA.columns  \n",
    "\n",
    "# Ordenem les característiques segons la seva importància (de major a menor)\n",
    "indices_ordenats = coef_importants.argsort()[::-1][0:25]\n",
    "\n",
    "# Mostrem els noms de les 20 característiques més importants amb la seva importància\n",
    "print(\"Les 20 característiques més importants són:\")\n",
    "for i in indices_ordenats:\n",
    "    print(f\"{feature_names[i]}: {coef_importants[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Farem el SD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Millor paràmetre: {'max_depth': 30, 'n_estimators': 100}\n",
      "Millor precisió: 88597.49076028807\n",
      "Mitjana dels resultats de test per cada combinació:\n",
      "91085.26934620566\n",
      "Les 20 característiques més importants són:\n",
      "SD318: 0.1763\n",
      "SD313: 0.0883\n",
      "SD3Narc: 0.0566\n",
      "SD3Mach: 0.0528\n",
      "SD301: 0.0467\n",
      "SD316: 0.0454\n",
      "SD309: 0.0446\n",
      "SD325: 0.0424\n",
      "SD305: 0.0402\n",
      "xSD325: 0.0400\n",
      "SD324: 0.0346\n",
      "SD304: 0.0280\n",
      "SD314: 0.0268\n",
      "SD323: 0.0234\n",
      "SD321: 0.0218\n",
      "xSD320: 0.0197\n",
      "SD308: 0.0181\n",
      "SD306: 0.0171\n",
      "SD315: 0.0169\n",
      "SD3Mach_3gr: 0.0160\n",
      "SD322: 0.0155\n",
      "SD303: 0.0137\n",
      "SD320: 0.0118\n",
      "SD310: 0.0111\n",
      "SD326: 0.0105\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100,250,500],\n",
    "    'max_depth': [30,50,75],\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    rf, \n",
    "    param_grid, \n",
    "    cv=10, \n",
    "    scoring='neg_mean_squared_error', \n",
    "    return_train_score=True\n",
    ")\n",
    "X_train_SD3 = filtered_datasets['SD3']['X_train_filtrat']\n",
    "y_train_SD3 = filtered_datasets['SD3']['y_train']\n",
    "\n",
    "# Realitzem el Grid Search\n",
    "grid_search.fit(X_train_SD3, y_train_SD3)\n",
    "\n",
    "# Resultats\n",
    "print(\"Millor paràmetre:\", grid_search.best_params_)\n",
    "print(\"Millor precisió:\", -grid_search.best_score_)  # Negatiu perquè s'ha usat neg_mean_squared_error\n",
    "\n",
    "# Mostrem la mitjana dels resultats de test\n",
    "print(\"Mitjana dels resultats de test per cada combinació:\")\n",
    "print(np.mean(grid_search.cv_results_['mean_test_score'] * -1))  # Multipliquem per -1 per obtenir el MSE positiu\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Obtenim la importància de cada característica\n",
    "coef_importants = best_model.feature_importances_\n",
    "\n",
    "# Obtenim els noms de les característiques (suposant que tenim un DataFrame 'df')\n",
    "feature_names = X_train_SD3.columns  \n",
    "\n",
    "# Ordenem les característiques segons la seva importància (de major a menor)\n",
    "indices_ordenats = coef_importants.argsort()[::-1][0:25]\n",
    "\n",
    "# Mostrem els noms de les 20 característiques més importants amb la seva importància\n",
    "print(\"Les 20 característiques més importants són:\")\n",
    "for i in indices_ordenats:\n",
    "    print(f\"{feature_names[i]}: {coef_importants[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Farem el ICUJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Millor paràmetre: {'max_depth': 75, 'n_estimators': 100}\n",
      "Millor precisió: 103867.471216\n",
      "Mitjana dels resultats de test per cada combinació:\n",
      "106314.23347673925\n",
      "Les 20 característiques més importants són:\n",
      "xICUT15: 0.1003\n",
      "xICUT03: 0.0960\n",
      "ICUJ02: 0.0621\n",
      "xICUT24: 0.0547\n",
      "ICUJsp9ExtrmN: 0.0336\n",
      "xICUT19: 0.0320\n",
      "ICUJ18: 0.0305\n",
      "ICUJ24: 0.0249\n",
      "xICUT23: 0.0244\n",
      "ICUTsp8SplitN: 0.0227\n",
      "xICUT14: 0.0223\n",
      "xICUJ19: 0.0220\n",
      "ICUJ11: 0.0200\n",
      "ICUTsp4SplitN: 0.0197\n",
      "ICUTsp4ExtrmN: 0.0195\n",
      "ICUJ12: 0.0194\n",
      "ICUJ08: 0.0176\n",
      "ICUJ14: 0.0175\n",
      "ICUJ22: 0.0168\n",
      "ICUJ19: 0.0158\n",
      "ICUJ20: 0.0154\n",
      "ICUJsp4SplitN: 0.0144\n",
      "xICUJ01: 0.0137\n",
      "xICUJ13: 0.0119\n",
      "xICUJ24: 0.0113\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100,250,500],\n",
    "    'max_depth': [30,50,75],\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    rf, \n",
    "    param_grid, \n",
    "    cv=10, \n",
    "    scoring='neg_mean_squared_error', \n",
    "    return_train_score=True\n",
    ")\n",
    "X_train_ICUJ = filtered_datasets['ICUJ']['X_train_filtrat']\n",
    "y_train_ICUJ = filtered_datasets['ICUJ']['y_train']\n",
    "\n",
    "# Realitzem el Grid Search\n",
    "grid_search.fit(X_train_ICUJ, y_train_ICUJ)\n",
    "\n",
    "# Resultats\n",
    "print(\"Millor paràmetre:\", grid_search.best_params_)\n",
    "print(\"Millor precisió:\", -grid_search.best_score_)  # Negatiu perquè s'ha usat neg_mean_squared_error\n",
    "\n",
    "# Mostrem la mitjana dels resultats de test\n",
    "print(\"Mitjana dels resultats de test per cada combinació:\")\n",
    "print(np.mean(grid_search.cv_results_['mean_test_score'] * -1))  # Multipliquem per -1 per obtenir el MSE positiu\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Obtenim la importància de cada característica\n",
    "coef_importants = best_model.feature_importances_\n",
    "\n",
    "# Obtenim els noms de les característiques (suposant que tenim un DataFrame 'df')\n",
    "feature_names = X_train_ICUJ.columns  \n",
    "\n",
    "# Ordenem les característiques segons la seva importància (de major a menor)\n",
    "indices_ordenats = coef_importants.argsort()[::-1][0:25]\n",
    "\n",
    "# Mostrem els noms de les 20 característiques més importants amb la seva importància\n",
    "print(\"Les 20 característiques més importants són:\")\n",
    "for i in indices_ordenats:\n",
    "    print(f\"{feature_names[i]}: {coef_importants[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Farem el ICUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Millor paràmetre: {'max_depth': 75, 'n_estimators': 100}\n",
      "Millor precisió: 96806.91756633333\n",
      "Mitjana dels resultats de test per cada combinació:\n",
      "98449.29728577925\n",
      "Les 20 característiques més importants són:\n",
      "ICUT24: 0.1024\n",
      "ICUT15: 0.1021\n",
      "ICUT03: 0.0835\n",
      "ICUT_UC: 0.0734\n",
      "ICUT23: 0.0666\n",
      "ICUT21: 0.0454\n",
      "ICUT_CA: 0.0419\n",
      "ICUT14: 0.0411\n",
      "ICUT_PT: 0.0409\n",
      "ICUT05: 0.0357\n",
      "ICUT07: 0.0327\n",
      "ICUT19: 0.0318\n",
      "ICUT_UE: 0.0314\n",
      "ICUT20: 0.0283\n",
      "ICUT06: 0.0269\n",
      "ICUT22: 0.0219\n",
      "ICUT02: 0.0215\n",
      "ICUT17: 0.0213\n",
      "ICUT09: 0.0204\n",
      "ICUT13: 0.0204\n",
      "ICUT08: 0.0196\n",
      "ICUT12: 0.0162\n",
      "ICUT_PT_3gr: 0.0148\n",
      "ICUT16: 0.0131\n",
      "ICUT_PT_2gr: 0.0115\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100,250,500],\n",
    "    'max_depth': [30,50,75],\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    rf, \n",
    "    param_grid, \n",
    "    cv=10, \n",
    "    scoring='neg_mean_squared_error', \n",
    "    return_train_score=True\n",
    ")\n",
    "X_train_ICUT = filtered_datasets['ICUT']['X_train_filtrat']\n",
    "y_train_ICUT = filtered_datasets['ICUT']['y_train']\n",
    "\n",
    "# Realitzem el Grid Search\n",
    "grid_search.fit(X_train_ICUT, y_train_ICUT)\n",
    "\n",
    "# Resultats\n",
    "print(\"Millor paràmetre:\", grid_search.best_params_)\n",
    "print(\"Millor precisió:\", -grid_search.best_score_)  # Negatiu perquè s'ha usat neg_mean_squared_error\n",
    "\n",
    "# Mostrem la mitjana dels resultats de test\n",
    "print(\"Mitjana dels resultats de test per cada combinació:\")\n",
    "print(np.mean(grid_search.cv_results_['mean_test_score'] * -1))  # Multipliquem per -1 per obtenir el MSE positiu\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Obtenim la importància de cada característica\n",
    "coef_importants = best_model.feature_importances_\n",
    "\n",
    "# Obtenim els noms de les característiques (suposant que tenim un DataFrame 'df')\n",
    "feature_names = X_train_ICUT.columns  \n",
    "\n",
    "# Ordenem les característiques segons la seva importància (de major a menor)\n",
    "indices_ordenats = coef_importants.argsort()[::-1][0:25]\n",
    "\n",
    "# Mostrem els noms de les 20 característiques més importants amb la seva importància\n",
    "print(\"Les 20 característiques més importants són:\")\n",
    "for i in indices_ordenats:\n",
    "    print(f\"{feature_names[i]}: {coef_importants[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculem el TRFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Millor paràmetre: {'max_depth': 75, 'n_estimators': 100}\n",
      "Millor precisió: 83513.9029901776\n",
      "Mitjana dels resultats de test per cada combinació:\n",
      "90497.30817980992\n",
      "Les 20 característiques més importants són:\n",
      "TRFMSocProbT: 0.1092\n",
      "TRFMWithDepT: 0.0797\n",
      "TRFMSocProbCat: 0.0530\n",
      "TRFM049: 0.0502\n",
      "TRFMAtteProb: 0.0492\n",
      "TRFMIntProb: 0.0298\n",
      "TRFM003: 0.0294\n",
      "TRFMAnxDepT: 0.0288\n",
      "TRFM069: 0.0288\n",
      "TRFMDSMStrProb: 0.0262\n",
      "TRFMIntProbT: 0.0253\n",
      "TRFM042: 0.0243\n",
      "TRFM092: 0.0235\n",
      "TRFMDSMStrProbT: 0.0235\n",
      "TRFM050: 0.0219\n",
      "TRFMAtteProbT: 0.0209\n",
      "TRFMWithDepCat: 0.0194\n",
      "TRFMRulBehT: 0.0172\n",
      "TRFM096: 0.0163\n",
      "TRFM031: 0.0163\n",
      "TRFM035: 0.0157\n",
      "TRFM001: 0.0152\n",
      "TRFM072: 0.0150\n",
      "TRFM017: 0.0127\n",
      "TRFMAnxDepCat: 0.0123\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100,250,500],\n",
    "    'max_depth': [30,50,75],\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    rf, \n",
    "    param_grid, \n",
    "    cv=10, \n",
    "    scoring='neg_mean_squared_error', \n",
    "    return_train_score=True\n",
    ")\n",
    "X_train_TRFM = filtered_datasets['TRFM']['X_train_filtrat']\n",
    "y_train_TRFM = filtered_datasets['TRFM']['y_train']\n",
    "\n",
    "# Realitzem el Grid Search\n",
    "grid_search.fit(X_train_TRFM, y_train_TRFM)\n",
    "\n",
    "# Resultats\n",
    "print(\"Millor paràmetre:\", grid_search.best_params_)\n",
    "print(\"Millor precisió:\", -grid_search.best_score_)  # Negatiu perquè s'ha usat neg_mean_squared_error\n",
    "\n",
    "# Mostrem la mitjana dels resultats de test\n",
    "print(\"Mitjana dels resultats de test per cada combinació:\")\n",
    "print(np.mean(grid_search.cv_results_['mean_test_score'] * -1))  # Multipliquem per -1 per obtenir el MSE positiu\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Obtenim la importància de cada característica\n",
    "coef_importants = best_model.feature_importances_\n",
    "\n",
    "# Obtenim els noms de les característiques (suposant que tenim un DataFrame 'df')\n",
    "feature_names = X_train_TRFM.columns  \n",
    "\n",
    "# Ordenem les característiques segons la seva importància (de major a menor)\n",
    "indices_ordenats = coef_importants.argsort()[::-1][0:25]\n",
    "\n",
    "# Mostrem els noms de les 20 característiques més importants amb la seva importància\n",
    "print(\"Les 20 característiques més importants són:\")\n",
    "for i in indices_ordenats:\n",
    "    print(f\"{feature_names[i]}: {coef_importants[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ara fem el dataset de TRFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Millor paràmetre: {'max_depth': 75, 'n_estimators': 500}\n",
      "Millor precisió: 109493.12376245335\n",
      "Mitjana dels resultats de test per cada combinació:\n",
      "113627.07636401037\n",
      "Les 20 característiques més importants són:\n",
      "TRFT_OP: 0.0608\n",
      "TRFT_AP: 0.0577\n",
      "TRFT100: 0.0482\n",
      "TRFMDSMAnxProbCat: 0.0456\n",
      "TRFT084: 0.0352\n",
      "TRFTDSMStrProbT: 0.0261\n",
      "TRFMDSMAnxProbT: 0.0235\n",
      "TRFTDSMStrProb: 0.0234\n",
      "TRFTSocProbT: 0.0228\n",
      "TRFMDSMAnxProb: 0.0219\n",
      "TRFT077: 0.0210\n",
      "TRFTDSMCDT: 0.0191\n",
      "TRFT098: 0.0190\n",
      "TRFT081: 0.0190\n",
      "TRFTDSMCD: 0.0178\n",
      "TRFT109: 0.0166\n",
      "TRFT019: 0.0157\n",
      "TRFT092: 0.0153\n",
      "TRFTDSMObsComT: 0.0152\n",
      "TRFTExtProbT: 0.0138\n",
      "TRFTDSMDepProb: 0.0133\n",
      "TRFTDSMObsCom: 0.0129\n",
      "TRFTRulBehT: 0.0127\n",
      "TRFTDSMADHDT: 0.0126\n",
      "TRFTExtProb: 0.0126\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100,250,500],\n",
    "    'max_depth': [30,50,75],\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    rf, \n",
    "    param_grid, \n",
    "    cv=10, \n",
    "    scoring='neg_mean_squared_error', \n",
    "    return_train_score=True\n",
    ")\n",
    "X_train_TRFT = filtered_datasets['TRFT']['X_train_filtrat']\n",
    "y_train_TRFT = filtered_datasets['TRFT']['y_train']\n",
    "\n",
    "# Realitzem el Grid Search\n",
    "grid_search.fit(X_train_TRFT, y_train_TRFT)\n",
    "\n",
    "# Resultats\n",
    "print(\"Millor paràmetre:\", grid_search.best_params_)\n",
    "print(\"Millor precisió:\", -grid_search.best_score_)  # Negatiu perquè s'ha usat neg_mean_squared_error\n",
    "\n",
    "# Mostrem la mitjana dels resultats de test\n",
    "print(\"Mitjana dels resultats de test per cada combinació:\")\n",
    "print(np.mean(grid_search.cv_results_['mean_test_score'] * -1))  # Multipliquem per -1 per obtenir el MSE positiu\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Obtenim la importància de cada característica\n",
    "coef_importants = best_model.feature_importances_\n",
    "\n",
    "# Obtenim els noms de les característiques (suposant que tenim un DataFrame 'df')\n",
    "feature_names = X_train_TRFT.columns  \n",
    "\n",
    "# Ordenem les característiques segons la seva importància (de major a menor)\n",
    "indices_ordenats = coef_importants.argsort()[::-1][0:25]\n",
    "\n",
    "# Mostrem els noms de les 20 característiques més importants amb la seva importància\n",
    "print(\"Les 20 característiques més importants són:\")\n",
    "for i in indices_ordenats:\n",
    "    print(f\"{feature_names[i]}: {coef_importants[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fem el dataset de YSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Millor paràmetre: {'max_depth': 75, 'n_estimators': 500}\n",
      "Millor precisió: 74527.84699845333\n",
      "Mitjana dels resultats de test per cada combinació:\n",
      "76506.03120982816\n",
      "Les 20 característiques més importants són:\n",
      "YSR041: 0.1091\n",
      "YSRTotProb: 0.0686\n",
      "YSR077: 0.0509\n",
      "YSRTotProbT: 0.0501\n",
      "YSRDSMDepProb: 0.0444\n",
      "YSRDSMADHD: 0.0392\n",
      "TRFMAnxDep: 0.0390\n",
      "YSRSomComp: 0.0389\n",
      "YSRPosQualT: 0.0387\n",
      "YSRDSMCD: 0.0310\n",
      "YSRSomCompT: 0.0278\n",
      "YSRThouProb: 0.0230\n",
      "YSRExtProbT: 0.0209\n",
      "YSR076: 0.0200\n",
      "YSRDSMODD: 0.0197\n",
      "YSRThouProbT: 0.0177\n",
      "YSR090: 0.0174\n",
      "YSR006: 0.0170\n",
      "YSRIntProbT: 0.0148\n",
      "YSRDSMStrProb: 0.0138\n",
      "YSR047: 0.0134\n",
      "YSRExtProb: 0.0129\n",
      "YSRDSMAnxProbT: 0.0121\n",
      "YSR017: 0.0117\n",
      "YSR064: 0.0110\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100,250,500],\n",
    "    'max_depth': [30,50,75],\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    rf, \n",
    "    param_grid, \n",
    "    cv=10, \n",
    "    scoring='neg_mean_squared_error', \n",
    "    return_train_score=True\n",
    ")\n",
    "X_train_YSR = filtered_datasets['YSR']['X_train_filtrat']\n",
    "y_train_YSR = filtered_datasets['YSR']['y_train']\n",
    "\n",
    "# Realitzem el Grid Search\n",
    "grid_search.fit(X_train_YSR, y_train_YSR)\n",
    "\n",
    "# Resultats\n",
    "print(\"Millor paràmetre:\", grid_search.best_params_)\n",
    "print(\"Millor precisió:\", -grid_search.best_score_)  # Negatiu perquè s'ha usat neg_mean_squared_error\n",
    "\n",
    "# Mostrem la mitjana dels resultats de test\n",
    "print(\"Mitjana dels resultats de test per cada combinació:\")\n",
    "print(np.mean(grid_search.cv_results_['mean_test_score'] * -1))  # Multipliquem per -1 per obtenir el MSE positiu\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Obtenim la importància de cada característica\n",
    "coef_importants = best_model.feature_importances_\n",
    "\n",
    "# Obtenim els noms de les característiques (suposant que tenim un DataFrame 'df')\n",
    "feature_names = X_train_YSR.columns  \n",
    "\n",
    "# Ordenem les característiques segons la seva importància (de major a menor)\n",
    "indices_ordenats = coef_importants.argsort()[::-1][0:25]\n",
    "\n",
    "# Mostrem els noms de les 20 característiques més importants amb la seva importància\n",
    "print(\"Les 20 característiques més importants són:\")\n",
    "for i in indices_ordenats:\n",
    "    print(f\"{feature_names[i]}: {coef_importants[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ara farem la triada fosca: Tri_PM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Millor paràmetre: {'max_depth': 75, 'n_estimators': 600}\n",
      "Millor precisió: 72141.48493877778\n",
      "Mitjana dels resultats de test per cada combinació:\n",
      "73308.12213091494\n",
      "Les 20 característiques més importants són:\n",
      "TriPMTotal: 0.1123\n",
      "TriPM15: 0.0909\n",
      "TriPM51: 0.0747\n",
      "TriPM12: 0.0678\n",
      "TriPMDisi: 0.0637\n",
      "xTriPM47: 0.0421\n",
      "TriPM19: 0.0307\n",
      "TriPM50: 0.0249\n",
      "xTriPM50: 0.0244\n",
      "TriPMBold: 0.0222\n",
      "TriPM37: 0.0220\n",
      "TriPM55: 0.0166\n",
      "TriPM03: 0.0157\n",
      "TriPM40: 0.0145\n",
      "xTriPM41: 0.0141\n",
      "TriPM41: 0.0138\n",
      "TriPM34: 0.0131\n",
      "TriPM35: 0.0121\n",
      "TriPM57: 0.0119\n",
      "TriPM01: 0.0118\n",
      "TriPM06: 0.0117\n",
      "TriPM29: 0.0107\n",
      "TriPM20: 0.0107\n",
      "TriPM22: 0.0104\n",
      "TriPM07: 0.0102\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [500,600,750],\n",
    "    'max_depth': [75,90,100],\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    rf, \n",
    "    param_grid, \n",
    "    cv=10, \n",
    "    scoring='neg_mean_squared_error', \n",
    "    return_train_score=True\n",
    ")\n",
    "X_train_TriPM = filtered_datasets['TriPM']['X_train_filtrat']\n",
    "y_train_TriPM = filtered_datasets['TriPM']['y_train']\n",
    "\n",
    "# Realitzem el Grid Search\n",
    "grid_search.fit(X_train_TriPM, y_train_TriPM)\n",
    "\n",
    "# Resultats\n",
    "print(\"Millor paràmetre:\", grid_search.best_params_)\n",
    "print(\"Millor precisió:\", -grid_search.best_score_)  # Negatiu perquè s'ha usat neg_mean_squared_error\n",
    "\n",
    "# Mostrem la mitjana dels resultats de test\n",
    "print(\"Mitjana dels resultats de test per cada combinació:\")\n",
    "print(np.mean(grid_search.cv_results_['mean_test_score'] * -1))  # Multipliquem per -1 per obtenir el MSE positiu\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Obtenim la importància de cada característica\n",
    "coef_importants = best_model.feature_importances_\n",
    "\n",
    "# Obtenim els noms de les característiques (suposant que tenim un DataFrame 'df')\n",
    "feature_names = X_train_TriPM.columns  \n",
    "\n",
    "# Ordenem les característiques segons la seva importància (de major a menor)\n",
    "indices_ordenats = coef_importants.argsort()[::-1][0:25]\n",
    "\n",
    "# Mostrem els noms de les 20 característiques més importants amb la seva importància\n",
    "print(\"Les 20 característiques més importants són:\")\n",
    "for i in indices_ordenats:\n",
    "    print(f\"{feature_names[i]}: {coef_importants[i]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
